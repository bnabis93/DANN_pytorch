{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from dataset.data_loader import GetLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from models.model import CNNModel\n",
    "import numpy as np\n",
    "from models.test import test\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: visdom in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (0.1.8.8)\n",
      "Requirement already satisfied: numpy>=1.8 in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from visdom) (1.14.5)\n",
      "Requirement already satisfied: torchfile in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from visdom) (0.1.0)\n",
      "Requirement already satisfied: six in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from visdom) (1.11.0)\n",
      "Requirement already satisfied: requests in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from visdom) (2.22.0)\n",
      "Requirement already satisfied: websocket-client in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from visdom) (0.56.0)\n",
      "Requirement already satisfied: tornado in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from visdom) (5.0.2)\n",
      "Requirement already satisfied: scipy in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from visdom) (1.1.0)\n",
      "Requirement already satisfied: pyzmq in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from visdom) (17.0.0)\n",
      "Requirement already satisfied: pillow in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from visdom) (5.1.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from requests->visdom) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from requests->visdom) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from requests->visdom) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages (from requests->visdom) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source image root :  dataset/MNIST\n",
      "target image root :  dataset/mnist_m\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Configuration block\n",
    "\n",
    "Question\n",
    "    what is manual seed?\n",
    "        Answer : https://discuss.pytorch.org/t/what-is-manual-seed/5939\n",
    "        Answer2 : https://pytorch.org/docs/master/torch.html?highlight=manual_seed#torch.manual_seed\n",
    "        manual seed is pytorch random number generators\n",
    "\n",
    "'''\n",
    "\n",
    "source_data_name = 'MNIST'\n",
    "target_data_name = 'mnist_m'\n",
    "source_image_root = os.path.join('dataset',source_data_name)\n",
    "target_image_root = os.path.join('dataset',target_data_name)\n",
    "model_root = 'models'\n",
    "cuda = True\n",
    "cudnn.benchmark = True\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "image_size = 28\n",
    "n_epoch = 100\n",
    "\n",
    "manual_seed = random.randint(1,10000)\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "print('source image root : ',source_image_root)\n",
    "print('target image root : ',target_image_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data configuration block\n",
    "\n",
    "torch vision trasnform documentation\n",
    "    documentation : https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "    Transforms are common image transformations. \n",
    "    'Compose'를 이용, 한번에 image chanege\n",
    "    \n",
    "    Transform Questions\n",
    "    1. normalize source mean, std values\n",
    "        Answer : https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457\n",
    "        \n",
    "    2. normalize target mean, std values\n",
    "'''\n",
    "\n",
    "img_transform_source = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean= (0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "img_transform_target = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/mnist_m/mnist_m_train_labels.txt\n",
      "source shape :  469\n",
      "target shape :  <torch.utils.data.dataloader.DataLoader object at 0x7f07a01df8d0>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data load block\n",
    "    Questions\n",
    "    1. what is DataLoader's num_worker, it is look like number of GPU\n",
    "        Answer (Guideline) : https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813\n",
    "        Answer2 (in documentation) : how many subprocesses to use for data loading. \n",
    "                                    0 means that the data will be loaded in the main process.\n",
    "                                    process를 얼마나 할당할것인지..?\n",
    "    2. Tatget data shuffle error\n",
    "        Answer (Now solved) : https://discuss.pytorch.org/t/dataloader-notimplemented-error/35349/4\n",
    "        Answer2 (Dataloader tutorial) : https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "        \n",
    "'''\n",
    "\n",
    "dataset_source = datasets.MNIST(\n",
    "    root = 'dataset',\n",
    "    train = True,\n",
    "    transform = img_transform_source,\n",
    "    download=True)\n",
    "\n",
    "dataloader_source = torch.utils.data.DataLoader(\n",
    "    dataset = dataset_source,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 4 )\n",
    "\n",
    "\n",
    "\n",
    "train_list = os.path.join(target_image_root, 'mnist_m_train_labels.txt')\n",
    "\n",
    "print(train_list)\n",
    "\n",
    "\n",
    "dataset_target = GetLoader(\n",
    "        data_root = os.path.join(target_image_root,'mnist_m_train'),\n",
    "        data_list = train_list,\n",
    "        transform = img_transform_target\n",
    ")\n",
    "\n",
    "dataloader_target = torch.utils.data.DataLoader(\n",
    "    dataset = dataset_target,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 4)\n",
    "\n",
    "print('source shape : ', len(dataloader_source))\n",
    "print('target shape : ', dataloader_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load model block\n",
    "&\n",
    "Setting optimizer and loss\n",
    "\n",
    "Question\n",
    "    1. explane model.parameters() and NLLLoss()\n",
    "    \n",
    "    Answer (model.parameters())\n",
    "        Trainable parameter list\n",
    "        example) for parameter in my_net.parameters():\n",
    "                    print(parameter)\n",
    "        https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325\n",
    "        \n",
    "    Answer2 (NLLLoss())\n",
    "        The negative log likelihood loss\n",
    "        https://pytorch.org/docs/stable/nn.html\n",
    "    \n",
    "    2. model.parameters().requires_grad?\n",
    "        If I want to freeze model weights\n",
    "        p.requires_grad = False\n",
    "        즉, 훈련을 시킬것인지 말 것인지를 결정하는 parameter\n",
    "        정확하게는 False시 backward를 하지 않는다.\n",
    "        \n",
    "        https://discuss.pytorch.org/t/model-train-and-requires-grad/25845 (pytorch forum)\n",
    "        https://pytorch.org/docs/stable/notes/autograd.html (documentation)\n",
    "\n",
    "'''\n",
    "\n",
    "my_net = CNNModel()\n",
    "\n",
    "optimizer = optim.Adam(my_net.parameters(), lr = lr)\n",
    "loss_class = torch.nn.NLLLoss()\n",
    "loss_domain = torch.nn.NLLLoss()\n",
    "\n",
    "if cuda :\n",
    "    my_net = my_net.cuda()\n",
    "    loss_class = loss_class.cuda()\n",
    "    loss_domain = loss_domain.cuda()\n",
    "    \n",
    "for p in my_net.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (f_conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (f_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (f_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (f_relu1): ReLU(inplace)\n",
      "  (f_conv2): Conv2d(64, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (f_bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (f_drop1): Dropout2d(p=0.5)\n",
      "  (f_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (f_relu2): ReLU(inplace)\n",
      ")\n",
      "Sequential(\n",
      "  (c_fc1): Linear(in_features=800, out_features=100, bias=True)\n",
      "  (c_bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (c_relu1): ReLU(inplace)\n",
      "  (c_drop1): Dropout2d(p=0.5)\n",
      "  (c_fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (c_bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (c_relu2): ReLU(inplace)\n",
      "  (c_fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (c_softmax): LogSoftmax()\n",
      ")\n",
      "Sequential(\n",
      "  (d_fc1): Linear(in_features=800, out_features=100, bias=True)\n",
      "  (d_bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (d_relu1): ReLU(inplace)\n",
      "  (d_fc2): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (d_softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(my_net.feature)\n",
    "print(my_net.class_classifier)\n",
    "print(my_net.domain_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " p value : 0.01 \n",
      " alpha : 0.049958374957880025\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 0, accuracy of the MNIST dataset: 0.984700\n",
      "epoch: 0, accuracy of the mnist_m dataset: 0.504611\n",
      " p value : 0.010021691973969632 \n",
      " alpha : 0.05006656354268846\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 1, accuracy of the MNIST dataset: 0.984300\n",
      "epoch: 1, accuracy of the mnist_m dataset: 0.525164\n",
      " p value : 0.010043383947939263 \n",
      " alpha : 0.05017475095252921\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 2, accuracy of the MNIST dataset: 0.989800\n",
      "epoch: 2, accuracy of the mnist_m dataset: 0.620931\n",
      " p value : 0.010065075921908893 \n",
      " alpha : 0.050282937184876086\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 3, accuracy of the MNIST dataset: 0.987700\n",
      "epoch: 3, accuracy of the mnist_m dataset: 0.668926\n",
      " p value : 0.010086767895878526 \n",
      " alpha : 0.0503911222372031\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 4, accuracy of the MNIST dataset: 0.986800\n",
      "epoch: 4, accuracy of the mnist_m dataset: 0.631708\n",
      " p value : 0.010108459869848157 \n",
      " alpha : 0.050499306106984054\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 5, accuracy of the MNIST dataset: 0.987800\n",
      "epoch: 5, accuracy of the mnist_m dataset: 0.649928\n",
      " p value : 0.010130151843817787 \n",
      " alpha : 0.05060748879169363\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 6, accuracy of the MNIST dataset: 0.990600\n",
      "epoch: 6, accuracy of the mnist_m dataset: 0.706255\n",
      " p value : 0.010151843817787418 \n",
      " alpha : 0.05071567028880564\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 7, accuracy of the MNIST dataset: 0.990000\n",
      "epoch: 7, accuracy of the mnist_m dataset: 0.768137\n",
      " p value : 0.010173535791757051 \n",
      " alpha : 0.05082385059579475\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 8, accuracy of the MNIST dataset: 0.990000\n",
      "epoch: 8, accuracy of the mnist_m dataset: 0.729697\n",
      " p value : 0.010195227765726681 \n",
      " alpha : 0.05093202971013544\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 9, accuracy of the MNIST dataset: 0.990600\n",
      "epoch: 9, accuracy of the mnist_m dataset: 0.751805\n",
      " p value : 0.010216919739696313 \n",
      " alpha : 0.05104020762930217\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 10, accuracy of the MNIST dataset: 0.990100\n",
      "epoch: 10, accuracy of the mnist_m dataset: 0.750028\n",
      " p value : 0.010238611713665942 \n",
      " alpha : 0.05114838435076985\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 11, accuracy of the MNIST dataset: 0.990800\n",
      "epoch: 11, accuracy of the mnist_m dataset: 0.777469\n",
      " p value : 0.010260303687635575 \n",
      " alpha : 0.051256559872013385\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 12, accuracy of the MNIST dataset: 0.991100\n",
      "epoch: 12, accuracy of the mnist_m dataset: 0.789579\n",
      " p value : 0.010281995661605207 \n",
      " alpha : 0.051364734190507466\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 13, accuracy of the MNIST dataset: 0.991000\n",
      "epoch: 13, accuracy of the mnist_m dataset: 0.747028\n",
      " p value : 0.010303687635574838 \n",
      " alpha : 0.05147290730372722\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 14, accuracy of the MNIST dataset: 0.991300\n",
      "epoch: 14, accuracy of the mnist_m dataset: 0.748139\n",
      " p value : 0.010325379609544468 \n",
      " alpha : 0.051581079209148006\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 15, accuracy of the MNIST dataset: 0.990700\n",
      "epoch: 15, accuracy of the mnist_m dataset: 0.793801\n",
      " p value : 0.0103470715835141 \n",
      " alpha : 0.05168924990424517\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 16, accuracy of the MNIST dataset: 0.989900\n",
      "epoch: 16, accuracy of the mnist_m dataset: 0.774692\n",
      " p value : 0.010368763557483732 \n",
      " alpha : 0.05179741938649429\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 17, accuracy of the MNIST dataset: 0.990500\n",
      "epoch: 17, accuracy of the mnist_m dataset: 0.748028\n",
      " p value : 0.010390455531453362 \n",
      " alpha : 0.051905587653370056\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 18, accuracy of the MNIST dataset: 0.989900\n",
      "epoch: 18, accuracy of the mnist_m dataset: 0.849906\n",
      " p value : 0.010412147505422993 \n",
      " alpha : 0.052013754702349146\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 19, accuracy of the MNIST dataset: 0.990500\n",
      "epoch: 19, accuracy of the mnist_m dataset: 0.820909\n",
      " p value : 0.010433839479392623 \n",
      " alpha : 0.052121920530906474\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 20, accuracy of the MNIST dataset: 0.989900\n",
      "epoch: 20, accuracy of the mnist_m dataset: 0.765248\n",
      " p value : 0.010455531453362256 \n",
      " alpha : 0.05223008513651872\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 21, accuracy of the MNIST dataset: 0.989000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21, accuracy of the mnist_m dataset: 0.799245\n",
      " p value : 0.010477223427331888 \n",
      " alpha : 0.05233824851666102\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 22, accuracy of the MNIST dataset: 0.991900\n",
      "epoch: 22, accuracy of the mnist_m dataset: 0.780802\n",
      " p value : 0.010498915401301517 \n",
      " alpha : 0.05244641066881006\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 23, accuracy of the MNIST dataset: 0.992500\n",
      "epoch: 23, accuracy of the mnist_m dataset: 0.838685\n",
      " p value : 0.010520607375271149 \n",
      " alpha : 0.05255457159044208\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 24, accuracy of the MNIST dataset: 0.991400\n",
      "epoch: 24, accuracy of the mnist_m dataset: 0.794134\n",
      " p value : 0.010542299349240782 \n",
      " alpha : 0.05266273127903287\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 25, accuracy of the MNIST dataset: 0.991500\n",
      "epoch: 25, accuracy of the mnist_m dataset: 0.826575\n",
      " p value : 0.010563991323210412 \n",
      " alpha : 0.052770889732059345\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 26, accuracy of the MNIST dataset: 0.991200\n",
      "epoch: 26, accuracy of the mnist_m dataset: 0.805688\n",
      " p value : 0.010585683297180043 \n",
      " alpha : 0.052879046946997965\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 27, accuracy of the MNIST dataset: 0.991900\n",
      "epoch: 27, accuracy of the mnist_m dataset: 0.824353\n",
      " p value : 0.010607375271149674 \n",
      " alpha : 0.05298720292132564\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 28, accuracy of the MNIST dataset: 0.991100\n",
      "epoch: 28, accuracy of the mnist_m dataset: 0.776914\n",
      " p value : 0.010629067245119306 \n",
      " alpha : 0.05309535765251838\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 29, accuracy of the MNIST dataset: 0.992500\n",
      "epoch: 29, accuracy of the mnist_m dataset: 0.828241\n",
      " p value : 0.010650759219088937 \n",
      " alpha : 0.053203511138053994\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 30, accuracy of the MNIST dataset: 0.991300\n",
      "epoch: 30, accuracy of the mnist_m dataset: 0.833352\n",
      " p value : 0.010672451193058569 \n",
      " alpha : 0.05331166337540916\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 31, accuracy of the MNIST dataset: 0.990500\n",
      "epoch: 31, accuracy of the mnist_m dataset: 0.820353\n",
      " p value : 0.010694143167028198 \n",
      " alpha : 0.053419814362061\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 32, accuracy of the MNIST dataset: 0.991500\n",
      "epoch: 32, accuracy of the mnist_m dataset: 0.809132\n",
      " p value : 0.010715835140997831 \n",
      " alpha : 0.05352796409548666\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 33, accuracy of the MNIST dataset: 0.992400\n",
      "epoch: 33, accuracy of the mnist_m dataset: 0.809688\n",
      " p value : 0.010737527114967463 \n",
      " alpha : 0.053636112573163475\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 34, accuracy of the MNIST dataset: 0.990000\n",
      "epoch: 34, accuracy of the mnist_m dataset: 0.845684\n",
      " p value : 0.010759219088937092 \n",
      " alpha : 0.05374425979256925\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 35, accuracy of the MNIST dataset: 0.992800\n",
      "epoch: 35, accuracy of the mnist_m dataset: 0.863793\n",
      " p value : 0.010780911062906724 \n",
      " alpha : 0.053852405751181776\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 36, accuracy of the MNIST dataset: 0.991400\n",
      "epoch: 36, accuracy of the mnist_m dataset: 0.851683\n",
      " p value : 0.010802603036876357 \n",
      " alpha : 0.05396055044647796\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 37, accuracy of the MNIST dataset: 0.990900\n",
      "epoch: 37, accuracy of the mnist_m dataset: 0.806133\n",
      " p value : 0.010824295010845987 \n",
      " alpha : 0.05406869387593649\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 38, accuracy of the MNIST dataset: 0.989800\n",
      "epoch: 38, accuracy of the mnist_m dataset: 0.852905\n",
      " p value : 0.010845986984815618 \n",
      " alpha : 0.05417683603703494\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 39, accuracy of the MNIST dataset: 0.991500\n",
      "epoch: 39, accuracy of the mnist_m dataset: 0.852572\n",
      " p value : 0.01086767895878525 \n",
      " alpha : 0.054284976927251316\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 40, accuracy of the MNIST dataset: 0.992700\n",
      "epoch: 40, accuracy of the mnist_m dataset: 0.870348\n",
      " p value : 0.01088937093275488 \n",
      " alpha : 0.05439311654406431\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 41, accuracy of the MNIST dataset: 0.993100\n",
      "epoch: 41, accuracy of the mnist_m dataset: 0.833685\n",
      " p value : 0.010911062906724512 \n",
      " alpha : 0.0545012548849515\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 42, accuracy of the MNIST dataset: 0.991700\n",
      "epoch: 42, accuracy of the mnist_m dataset: 0.866904\n",
      " p value : 0.010932754880694144 \n",
      " alpha : 0.054609391947392005\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43, accuracy of the MNIST dataset: 0.991500\n",
      "epoch: 43, accuracy of the mnist_m dataset: 0.859127\n",
      " p value : 0.010954446854663773 \n",
      " alpha : 0.05471752772886407\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 44, accuracy of the MNIST dataset: 0.992100\n",
      "epoch: 44, accuracy of the mnist_m dataset: 0.853683\n",
      " p value : 0.010976138828633405 \n",
      " alpha : 0.05482566222684637\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 45, accuracy of the MNIST dataset: 0.990500\n",
      "epoch: 45, accuracy of the mnist_m dataset: 0.853905\n",
      " p value : 0.010997830802603038 \n",
      " alpha : 0.054933795438818045\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 46, accuracy of the MNIST dataset: 0.993200\n",
      "epoch: 46, accuracy of the mnist_m dataset: 0.855016\n",
      " p value : 0.011019522776572668 \n",
      " alpha : 0.05504192736225755\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 47, accuracy of the MNIST dataset: 0.991600\n",
      "epoch: 47, accuracy of the mnist_m dataset: 0.834574\n",
      " p value : 0.011041214750542299 \n",
      " alpha : 0.05515005799464401\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 48, accuracy of the MNIST dataset: 0.991900\n",
      "epoch: 48, accuracy of the mnist_m dataset: 0.854794\n",
      " p value : 0.01106290672451193 \n",
      " alpha : 0.055258187333457\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 49, accuracy of the MNIST dataset: 0.992800\n",
      "epoch: 49, accuracy of the mnist_m dataset: 0.879458\n",
      " p value : 0.011084598698481562 \n",
      " alpha : 0.05536631537617542\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 50, accuracy of the MNIST dataset: 0.992000\n",
      "epoch: 50, accuracy of the mnist_m dataset: 0.873236\n",
      " p value : 0.011106290672451193 \n",
      " alpha : 0.05547444212027863\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 51, accuracy of the MNIST dataset: 0.992800\n",
      "epoch: 51, accuracy of the mnist_m dataset: 0.871459\n",
      " p value : 0.011127982646420825 \n",
      " alpha : 0.05558256756324664\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 52, accuracy of the MNIST dataset: 0.991800\n",
      "epoch: 52, accuracy of the mnist_m dataset: 0.873125\n",
      " p value : 0.011149674620390454 \n",
      " alpha : 0.05569069170255858\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 53, accuracy of the MNIST dataset: 0.991800\n",
      "epoch: 53, accuracy of the mnist_m dataset: 0.863904\n",
      " p value : 0.011171366594360087 \n",
      " alpha : 0.055798814535694685\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 54, accuracy of the MNIST dataset: 0.991500\n",
      "epoch: 54, accuracy of the mnist_m dataset: 0.794356\n",
      " p value : 0.011193058568329719 \n",
      " alpha : 0.055906936060134305\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 55, accuracy of the MNIST dataset: 0.991600\n",
      "epoch: 55, accuracy of the mnist_m dataset: 0.866681\n",
      " p value : 0.011214750542299349 \n",
      " alpha : 0.0560150562733579\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 56, accuracy of the MNIST dataset: 0.993300\n",
      "epoch: 56, accuracy of the mnist_m dataset: 0.833130\n",
      " p value : 0.01123644251626898 \n",
      " alpha : 0.056123175172845485\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 57, accuracy of the MNIST dataset: 0.992400\n",
      "epoch: 57, accuracy of the mnist_m dataset: 0.863126\n",
      " p value : 0.011258134490238613 \n",
      " alpha : 0.0562312927560773\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 58, accuracy of the MNIST dataset: 0.992500\n",
      "epoch: 58, accuracy of the mnist_m dataset: 0.862793\n",
      " p value : 0.011279826464208243 \n",
      " alpha : 0.056339409020533804\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 59, accuracy of the MNIST dataset: 0.992300\n",
      "epoch: 59, accuracy of the mnist_m dataset: 0.854683\n",
      " p value : 0.011301518438177874 \n",
      " alpha : 0.056447523963695456\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 60, accuracy of the MNIST dataset: 0.990300\n",
      "epoch: 60, accuracy of the mnist_m dataset: 0.770692\n",
      " p value : 0.011323210412147506 \n",
      " alpha : 0.05655563758304294\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 61, accuracy of the MNIST dataset: 0.991300\n",
      "epoch: 61, accuracy of the mnist_m dataset: 0.847572\n",
      " p value : 0.011344902386117137 \n",
      " alpha : 0.05666374987605716\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 62, accuracy of the MNIST dataset: 0.991000\n",
      "epoch: 62, accuracy of the mnist_m dataset: 0.861238\n",
      " p value : 0.011366594360086768 \n",
      " alpha : 0.05677186084021857\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 63, accuracy of the MNIST dataset: 0.990300\n",
      "epoch: 63, accuracy of the mnist_m dataset: 0.851794\n",
      " p value : 0.0113882863340564 \n",
      " alpha : 0.0568799704730083\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 64, accuracy of the MNIST dataset: 0.990400\n",
      "epoch: 64, accuracy of the mnist_m dataset: 0.865459\n",
      " p value : 0.01140997830802603 \n",
      " alpha : 0.056988078771907924\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 65, accuracy of the MNIST dataset: 0.990600\n",
      "epoch: 65, accuracy of the mnist_m dataset: 0.836685\n",
      " p value : 0.011431670281995661 \n",
      " alpha : 0.05709618573439834\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 66, accuracy of the MNIST dataset: 0.992000\n",
      "epoch: 66, accuracy of the mnist_m dataset: 0.876347\n",
      " p value : 0.011453362255965294 \n",
      " alpha : 0.0572042913579609\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 67, accuracy of the MNIST dataset: 0.991800\n",
      "epoch: 67, accuracy of the mnist_m dataset: 0.858238\n",
      " p value : 0.011475054229934924 \n",
      " alpha : 0.057312395640077174\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 68, accuracy of the MNIST dataset: 0.991100\n",
      "epoch: 68, accuracy of the mnist_m dataset: 0.873459\n",
      " p value : 0.011496746203904555 \n",
      " alpha : 0.05742049857822917\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 69, accuracy of the MNIST dataset: 0.990500\n",
      "epoch: 69, accuracy of the mnist_m dataset: 0.834018\n",
      " p value : 0.011518438177874187 \n",
      " alpha : 0.057528600169898025\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 70, accuracy of the MNIST dataset: 0.991100\n",
      "epoch: 70, accuracy of the mnist_m dataset: 0.872125\n",
      " p value : 0.011540130151843818 \n",
      " alpha : 0.05763670041256597\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 71, accuracy of the MNIST dataset: 0.988900\n",
      "epoch: 71, accuracy of the mnist_m dataset: 0.883568\n",
      " p value : 0.01156182212581345 \n",
      " alpha : 0.05774479930371501\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 72, accuracy of the MNIST dataset: 0.991100\n",
      "epoch: 72, accuracy of the mnist_m dataset: 0.883791\n",
      " p value : 0.01158351409978308 \n",
      " alpha : 0.05785289684082717\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 73, accuracy of the MNIST dataset: 0.990400\n",
      "epoch: 73, accuracy of the mnist_m dataset: 0.851128\n",
      " p value : 0.01160520607375271 \n",
      " alpha : 0.05796099302138491\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 74, accuracy of the MNIST dataset: 0.992300\n",
      "epoch: 74, accuracy of the mnist_m dataset: 0.885457\n",
      " p value : 0.011626898047722344 \n",
      " alpha : 0.05806908784287046\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 75, accuracy of the MNIST dataset: 0.990900\n",
      "epoch: 75, accuracy of the mnist_m dataset: 0.838240\n",
      " p value : 0.011648590021691975 \n",
      " alpha : 0.0581771813027665\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 76, accuracy of the MNIST dataset: 0.989900\n",
      "epoch: 76, accuracy of the mnist_m dataset: 0.890457\n",
      " p value : 0.011670281995661605 \n",
      " alpha : 0.05828527339855549\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 77, accuracy of the MNIST dataset: 0.991000\n",
      "epoch: 77, accuracy of the mnist_m dataset: 0.877458\n",
      " p value : 0.011691973969631236 \n",
      " alpha : 0.058393364127720115\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 78, accuracy of the MNIST dataset: 0.991500\n",
      "epoch: 78, accuracy of the mnist_m dataset: 0.876569\n",
      " p value : 0.01171366594360087 \n",
      " alpha : 0.058501453487743715\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 79, accuracy of the MNIST dataset: 0.990800\n",
      "epoch: 79, accuracy of the mnist_m dataset: 0.865793\n",
      " p value : 0.011735357917570499 \n",
      " alpha : 0.05860954147610897\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 80, accuracy of the MNIST dataset: 0.991300\n",
      "epoch: 80, accuracy of the mnist_m dataset: 0.872125\n",
      " p value : 0.01175704989154013 \n",
      " alpha : 0.058717628090299234\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 81, accuracy of the MNIST dataset: 0.991900\n",
      "epoch: 81, accuracy of the mnist_m dataset: 0.899789\n",
      " p value : 0.01177874186550976 \n",
      " alpha : 0.0588257133277974\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 82, accuracy of the MNIST dataset: 0.988900\n",
      "epoch: 82, accuracy of the mnist_m dataset: 0.872459\n",
      " p value : 0.011800433839479393 \n",
      " alpha : 0.05893379718608749\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 83, accuracy of the MNIST dataset: 0.991800\n",
      "epoch: 83, accuracy of the mnist_m dataset: 0.869237\n",
      " p value : 0.011822125813449024 \n",
      " alpha : 0.05904187966265284\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 84, accuracy of the MNIST dataset: 0.992300\n",
      "epoch: 84, accuracy of the mnist_m dataset: 0.869792\n",
      " p value : 0.011843817787418656 \n",
      " alpha : 0.059149960754976805\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 85, accuracy of the MNIST dataset: 0.992000\n",
      "epoch: 85, accuracy of the mnist_m dataset: 0.885124\n",
      " p value : 0.011865509761388286 \n",
      " alpha : 0.05925804046054339\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 86, accuracy of the MNIST dataset: 0.990700\n",
      "epoch: 86, accuracy of the mnist_m dataset: 0.870126\n",
      " p value : 0.011887201735357919 \n",
      " alpha : 0.05936611877683662\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 87, accuracy of the MNIST dataset: 0.990900\n",
      "epoch: 87, accuracy of the mnist_m dataset: 0.862126\n",
      " p value : 0.01190889370932755 \n",
      " alpha : 0.059474195701340715\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 88, accuracy of the MNIST dataset: 0.991400\n",
      "epoch: 88, accuracy of the mnist_m dataset: 0.869681\n",
      " p value : 0.01193058568329718 \n",
      " alpha : 0.059582271231539474\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 89, accuracy of the MNIST dataset: 0.991500\n",
      "epoch: 89, accuracy of the mnist_m dataset: 0.885346\n",
      " p value : 0.011952277657266811 \n",
      " alpha : 0.059690345364917574\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 90, accuracy of the MNIST dataset: 0.990700\n",
      "epoch: 90, accuracy of the mnist_m dataset: 0.891901\n",
      " p value : 0.01197396963123644 \n",
      " alpha : 0.05979841809895925\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 91, accuracy of the MNIST dataset: 0.992600\n",
      "epoch: 91, accuracy of the mnist_m dataset: 0.896456\n",
      " p value : 0.011995661605206074 \n",
      " alpha : 0.05990648943114918\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 92, accuracy of the MNIST dataset: 0.992500\n",
      "epoch: 92, accuracy of the mnist_m dataset: 0.882569\n",
      " p value : 0.012017353579175705 \n",
      " alpha : 0.06001455935897182\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 93, accuracy of the MNIST dataset: 0.990300\n",
      "epoch: 93, accuracy of the mnist_m dataset: 0.907455\n",
      " p value : 0.012039045553145335 \n",
      " alpha : 0.060122627879912516\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 94, accuracy of the MNIST dataset: 0.991600\n",
      "epoch: 94, accuracy of the mnist_m dataset: 0.892234\n",
      " p value : 0.012060737527114966 \n",
      " alpha : 0.06023069499145617\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 95, accuracy of the MNIST dataset: 0.990600\n",
      "epoch: 95, accuracy of the mnist_m dataset: 0.860904\n",
      " p value : 0.0120824295010846 \n",
      " alpha : 0.060338760691087456\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 96, accuracy of the MNIST dataset: 0.992500\n",
      "epoch: 96, accuracy of the mnist_m dataset: 0.890346\n",
      " p value : 0.01210412147505423 \n",
      " alpha : 0.06044682497629217\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 97, accuracy of the MNIST dataset: 0.989500\n",
      "epoch: 97, accuracy of the mnist_m dataset: 0.892790\n",
      " p value : 0.01212581344902386 \n",
      " alpha : 0.06055488784455543\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 98, accuracy of the MNIST dataset: 0.991400\n",
      "epoch: 98, accuracy of the mnist_m dataset: 0.898456\n",
      " p value : 0.012147505422993492 \n",
      " alpha : 0.06066294929336258\n",
      " size of s_img : torch.Size([128, 1, 28, 28]) and s_label : torch.Size([128]): \n",
      " batch size :  128\n",
      " input_img shape  torch.Size([128, 1, 28, 28])\n",
      " size of t_img : torch.Size([128, 3, 28, 28])\n",
      " batch size :  128\n",
      "epoch: 99, accuracy of the MNIST dataset: 0.991700\n",
      "epoch: 99, accuracy of the mnist_m dataset: 0.896234\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train block\n",
    "\n",
    "Questions\n",
    "    1. I don't know what is 'p' & 'alpha' and how can I calculate it.\n",
    "    Answer (code 분석)\n",
    "        p는 alpha에 영향을 미치는 hyperparameter 따라서 alpha를 보면 된다.\n",
    "        alpha는 model에 입력되는 hyperparameter이다.\n",
    "        alpha -> ReverseLayerF.apply(feature, alpha), model.py -> forward / backward propagation시 영향을 미침\n",
    "        논문의 \\gamma로 추정이 되나, 아직 확실하게 모르겠다.\n",
    "        paper의 adaptation param을 보자.\n",
    "    \n",
    "    2. 학습 순서에 관하여 정리(source / target -> class / domain claasifier)\n",
    "        순서는 source -> target 순서로 학습이 진행된다.\n",
    "        Source -> class / domain error\n",
    "            a. model로 부터 output을 받는다.\n",
    "            b. 얻은 output을 바탕으로 class / domain loss를 얻는다.\n",
    "        Target -> class / domain error\n",
    "            a. model로부터 output을 받는다.\n",
    "            b. 얻은 output을 바탕으로 class / domain loss 계산\n",
    "        Source의 loss와 target의 loss를 바탕으로 최종 loss를 계산한다.\n",
    "         \n",
    "    3. keras처럼 깔끔히 결과를 json 혹은 다른 형태로 저장 할 수있는 방법? (keras와 호환이 가능하게...?)\n",
    "        Answer1. (Official documentation)\n",
    "            torch.save / torch.load를 이용하여 저장 / 불러오기를 한다.\n",
    "            공식 문서에서는 state_dict을 이용하여 저장하는 것을 권장하고 있다.\n",
    "            state_dict은 model의 learnable parameter와 registered buffers를 저장해놓은 dictionary이다.\n",
    "            여튼 tensor의 정보가 들어있는 dictionary 형태.\n",
    "            print(my_net.state_dict) 를 통하여 어떤 정보가 들어있을 것이란것을 알 수 있다.\n",
    "            \n",
    "        Reference : https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        \n",
    "        Answer2. (Bestway to save pytorch model)\n",
    "            state_dict 형태로 저장하는것이 가장 좋다는 의견이 많다.\n",
    "            하지만 나는 keras와 혼용이 가능한 형태로 저장하는 방법을 찾아보려고 한다.\n",
    "            Ref2에 의하면 param / output등이 각각 keras / torch에 최적화 되어있기 때문에 \n",
    "            torch -> keras는 별로 좋지 않은 생각이라고들 한다. (2017)\n",
    "            output에서 차이가 존재하는 것 같다.\n",
    "            별도의 converter로 변환을 시켜줘야 하는 것 같다.\n",
    "            model param의 경우 단순 수의 나열이므로 호환이 될 것같은데 좀 더 찾아봐야 할 듯...\n",
    "                  \n",
    "        Reference : https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch (bestway to save)\n",
    "        Reference2 : https://discuss.pytorch.org/t/transferring-weights-from-keras-to-pytorch/9889/3 (torch -> keras)\n",
    "        \n",
    "\n",
    "    4. err.backward, optimizer.step의 documentation을 보고 pytorch 의 진행을 알아보자\n",
    "        Answer\n",
    "            Just compute gradient.\n",
    "            loss.backward() -> computes dloss/dx (미분값), x.grad 계산\n",
    "            optimizer.step() -> updates the value of 'x' using the gradient x.grad 우리가 설정한 optimizer에 맞추어\n",
    "            \n",
    "            Reference : https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944\n",
    "            (What does the backward() function do?)\n",
    "    \n",
    "    5. 중간중간에 copy는 왜하나...? 데이터가 변환되는 것을 막기위해...?\n",
    "        Answer\n",
    "            음... 이 코드에서는 단순 데이터 삽입을 위한 copy로 보인다.\n",
    "            input_img에서 data container를 생성하고\n",
    "            s_img를 옮기는 단순 삽입을 위한 copy.\n",
    "        \n",
    "'''\n",
    "\n",
    "# average err\n",
    "class_s_err = []\n",
    "source_err = []\n",
    "target_err = []\n",
    "final_err = []\n",
    "\n",
    "source_acc_list = []\n",
    "target_acc_list = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    len_dataloader = min(len(dataloader_source), len(dataloader_target))\n",
    "    data_source_iter = iter(dataloader_source)\n",
    "    data_target_iter = iter(dataloader_target)\n",
    "    \n",
    "    i =0\n",
    "    \n",
    "    temp_class_s_err = []\n",
    "    temp_source_err = []\n",
    "    temp_target_err = []\n",
    "    temp_final_err = []\n",
    "    temp_s_acc = None\n",
    "    temp_t_acc = None\n",
    "    \n",
    "    while i<len_dataloader:\n",
    "        \n",
    "        p = float(i+epoch+len_dataloader) / n_epoch / len_dataloader\n",
    "        alpha = 2. / (1. + np.exp(-10*p)) -1\n",
    "        \n",
    "        ################# source DANN ##################################\n",
    "        \n",
    "        data_source = data_source_iter.next()\n",
    "        s_img, s_label = data_source\n",
    "        \n",
    "        my_net.zero_grad()\n",
    "        batch_size = len(s_label)\n",
    "        \n",
    "        input_img = torch.FloatTensor(batch_size, 3, image_size,image_size)\n",
    "        class_label = torch.LongTensor(batch_size)\n",
    "        domain_label = torch.zeros(batch_size)\n",
    "        domain_label = domain_label.long()\n",
    "            \n",
    "        if cuda:\n",
    "            s_img = s_img.cuda()\n",
    "            s_label = s_label.cuda()\n",
    "            input_img = input_img.cuda()\n",
    "            class_label = class_label.cuda()\n",
    "            domain_label = domain_label.cuda()\n",
    "        \n",
    "        input_img.resize_as_(s_img).copy_(s_img)\n",
    "        class_label.resize_as_(s_label).copy_(s_label)\n",
    "        \n",
    "        if (i ==0):\n",
    "            print(' p value : {} \\n alpha : {}'.format(p, alpha))\n",
    "            print(' size of s_img : {} and s_label : {}: '.format(np.shape(s_img), np.shape(s_label)))\n",
    "            print(' batch size : ', batch_size)\n",
    "            print(' input_img shape ',np.shape(input_img))\n",
    "        \n",
    "        \n",
    "        class_output, domain_output = my_net(input_data= input_img, alpha = alpha)\n",
    "        err_s_label = loss_class(class_output, class_label)\n",
    "        err_s_domain = loss_domain(domain_output, domain_label)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ################################################################\n",
    "        \n",
    "        ################# Target DANN ##################################\n",
    "        \n",
    "        data_target =data_target_iter.next()\n",
    "        t_img, _ = data_target\n",
    "        batch_size = len(t_img)\n",
    "        \n",
    "        if (i ==0):\n",
    "            print(' size of t_img : {}'.format(np.shape(t_img)))\n",
    "            print(' batch size : ', batch_size)\n",
    "        \n",
    "        input_img = torch.FloatTensor(batch_size, 3, image_size,image_size)\n",
    "        domain_label = torch.ones(batch_size)\n",
    "        domain_label = domain_label.long()\n",
    "        \n",
    "        if cuda:\n",
    "            t_img = t_img.cuda()\n",
    "            input_img = input_img.cuda()\n",
    "            domain_label = domain_label.cuda()\n",
    "        \n",
    "        input_img.resize_as_(t_img).copy_(t_img)\n",
    "        \n",
    "        _, domain_output = my_net(input_data = input_img, alpha = alpha)\n",
    "        err_t_domain = loss_domain(domain_output, domain_label)\n",
    "        err = err_t_domain + err_s_domain + err_s_label\n",
    "        \n",
    "        temp_class_s_err.append(err_s_label)\n",
    "        temp_source_err.append(err_s_domain)\n",
    "        temp_target_err.append(err_t_domain)\n",
    "        temp_final_err.append(err)\n",
    "        \n",
    "        err.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        i = i+1\n",
    "        \n",
    "        '''\n",
    "        print ('epoch: %d, [iter: %d / all %d], err_s_label: %f, err_s_domain: %f, err_t_domain: %f' \\\n",
    "              % (epoch, i, len_dataloader, err_s_label.data.cpu().numpy(),\n",
    "                 err_s_domain.data.cpu().numpy(), err_t_domain.data.cpu().item()))\n",
    "                 '''\n",
    "        \n",
    "    temp_class_s_err = np.asarray(temp_class_s_err,dtype='float32')\n",
    "    temp_source_err = np.asarray(temp_source_err, dtype = 'float32')\n",
    "    temp_target_err = np.asarray(temp_target_err, dtype='float32')\n",
    "    temp_final_err = np.asarray(temp_final_err, dtype= 'float32')\n",
    "    \n",
    "    class_s_err.append(np.mean(temp_class_s_err))\n",
    "    source_err.append(np.mean(temp_source_err))\n",
    "    target_err.append(np.mean(temp_target_err))\n",
    "    final_err.append(np.mean(temp_final_err))\n",
    "    \n",
    "    temp_s_acc = test(source_data_name, epoch)\n",
    "    temp_t_acc = test(target_data_name, epoch)\n",
    "    \n",
    "    source_acc_list.append(temp_s_acc)\n",
    "    target_acc_list.append(temp_t_acc)\n",
    "    \n",
    "    torch.save(my_net, '{0}/mnist_mnistm_model_epoch_{1}.pth'.format(model_root, epoch))\n",
    "        \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.state_dict of CNNModel(\n",
      "  (feature): Sequential(\n",
      "    (f_conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (f_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (f_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (f_relu1): ReLU(inplace)\n",
      "    (f_conv2): Conv2d(64, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (f_bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (f_drop1): Dropout2d(p=0.5)\n",
      "    (f_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (f_relu2): ReLU(inplace)\n",
      "  )\n",
      "  (class_classifier): Sequential(\n",
      "    (c_fc1): Linear(in_features=800, out_features=100, bias=True)\n",
      "    (c_bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (c_relu1): ReLU(inplace)\n",
      "    (c_drop1): Dropout2d(p=0.5)\n",
      "    (c_fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (c_bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (c_relu2): ReLU(inplace)\n",
      "    (c_fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      "    (c_softmax): LogSoftmax()\n",
      "  )\n",
      "  (domain_classifier): Sequential(\n",
      "    (d_fc1): Linear(in_features=800, out_features=100, bias=True)\n",
      "    (d_bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (d_relu1): ReLU(inplace)\n",
      "    (d_fc2): Linear(in_features=100, out_features=2, bias=True)\n",
      "    (d_softmax): LogSoftmax()\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Output visualization block\n",
    "\n",
    "Question.\n",
    "    1. What is the best way to plot model's output?\n",
    "    \n",
    "    2. Visdom?\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4664506  0.764782   0.7932825  0.7871739  0.7527436  0.72873586\n",
      " 0.71992344 0.7046467  0.7280016  0.7382946  0.7346776  0.7498563\n",
      " 0.7473987  0.7819215  0.76265866 0.77906865 0.7607271  0.77463174\n",
      " 0.7960924  0.80098313 0.78019226 0.7937581  0.79418397 0.82238346\n",
      " 0.80297613 0.7973706  0.8269827  0.84966236 0.8223599  0.87112564\n",
      " 0.8205824  0.8452183  0.8474128  0.8472687  0.8709221  0.8568044\n",
      " 0.89463663 0.8972957  0.8734867  0.8755146  0.89869577 0.88380706\n",
      " 0.8881548  0.91308594 0.8996075  0.90340215 0.89814186 0.92118615\n",
      " 0.9234037  0.9083738  0.9076234  0.9360166  0.9305311  0.9188406\n",
      " 0.91323686 0.9386805  0.94473845 0.92408204 0.95639145 0.92231923\n",
      " 0.9412118  0.93662286 0.9442724  0.9371314  0.9635582  0.9461233\n",
      " 0.9394743  0.9502348  0.9354781  0.9484004  0.9344546  0.9755428\n",
      " 0.9541262  0.94522744 0.9402025  0.95849913 0.9710459  0.9614248\n",
      " 0.95849156 0.9530995  0.9931307  0.9732873  0.9669984  0.9480736\n",
      " 0.9627732  0.9726642  0.990308   0.96793723 0.9788242  0.9847379\n",
      " 0.99051034 0.99601716 0.98082066 0.98932743 0.99853146 0.9883203\n",
      " 1.0125003  1.0079366  1.002663   1.0035219 ]\n"
     ]
    }
   ],
   "source": [
    "print(final_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEhCAYAAABC/rboAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XucHGWZ9//Pd4YOmQTIBMITyZADshgEOQQisOJqUBTwAFlxBcRDBBdXhBXErGGXBUT2MZgHhFVWQeSHog+EQ5yNghtcYZZHFCQxwRgwghAgTQgImRwHMofr90dVDzU91edTTc/1fr3mNd1VNV1X19TdV9+HuktmhnPOOVdtLY0OwDnnXHPyBOOcc64mPME455yrCU8wzjnnasITjHPOuZrwBOOcc64mPMFUmaRpkrZJaq3Ca90i6cpqxOVGr0adR5J+LunTVX7NeZJ+Vc3XzHr9ITFLulLSXyS9WM2yPVrs0ugARipJ64DJQH9k8VvM7Dlgt4YE5VyCmNlJjY6hVNGYJU0DLgKmm9lL4WIv2yXwGkxlPmxmu0V+Xmh0QHEkDfsiEbeswGtIkp8vbjSZBrwSSS5lK7W81Uq94/APjCqTNEOSZf6RkrokfU3SQ5K2SrpP0qTI9neG1e/Nkh6UdHAJ+zpL0hOSNklaJml6ZJ1J+oKkJ4En8yx7h6RHw/0/KukdkdfokvRvkh4CdgBvrvT4uNqTNEvS78LzbTEwNmv930t6StKrkpZKmhJZZ5LOlfRk+Pdfk7S/pF9L2iLpDkljwm0nSvqZpJfDc/BnkvaNvFaXpM+Gj+dJ+pWk/xNu+4yknDUcSVMlLQlf+xVJ386x3XWSng9jWyHpbyLrjpK0PFy3UdI14fKxkn4Uvm53eN5PjsYs6XjgF8CUsFnslpiyPUHS9yVtkJQOm9NaI+/3IUnflPQKcHkR/7d5kp4Oj/szks4Ml7dIukTSs5JekvRDSRPCdXMkrc96nXVh/Ei6XNJd4fvdAsyT1CrpnyX9OdzXCklTw+0PlPSL8NxYK+ljheLOy8z8p4wfYB1wfMzyGYABu4TPu4A/A28B2sLnCyPbnwXsDuwKXAusiqy7Bbgyx/5PAZ4C3krQ1HkJ8OvIeiMoIHsCbXHLwt+bgE+Gr3FG+HyvSOzPAQeH61ONPu7+U/C8HAM8C1wIpICPAr2Z8wh4D/AX4IjwnPsW8GDWefOfwB7h//114JcEXy4mAI8Dnw633Qs4FRgXnsN3Ap2R1+oCPhs+nhfG8fdAK/B54AVAMe+hFXgM+CYwniBBvjPyOr+KbPuJMI5dCJqzXgTGhut+A3wyfLwbcEz4+HPAT8O4W4EjgT1iYp4DrI/sawZDy/ZPgBvCGP8X8Fvgc5E4+4Dzw9jaCvzfxgNbgJnh832Ag8PHZxGU9TeH72MJcGtcjOGydYSfTQSJrReYS1ChaAPmA6uBmYCAw8JjOB54HvhMGPMsgnPloLLPx0YXiJH6E/4TtwHd4U9njpOwC7gk8nfnAv+V4zXbw7+dED6/hdwJ5ufA2ZHnLQS1jOnhcwPek/U3Q5YRJJbfZm3zG2BeJPYrGn2s/aek8/JdZH1wA7/mjQTzfeAbkXW7hR9AMyLnyLGR9SuAr0SeXw1cm2PfhwObIs+7GJpgnoqsGxfu600xr/PXwMuZMpS1bh6RBBOzfhNwWPj4QeCrwKSsbc4Kj8mhMX8fjXkOORIMQf/r60QSB8EXtAcicT5Xwv9tfPg5cipZyYggwZ8beT4z/J/tkh1juH4dQxPMg1nr1wKnxMRwGvD/spbdAFxW7vnoTWSVmWtm7eHP3DzbvRh5vIOwozCsqi4Mq6pbCE4MgEkUNh24LqzidwOvEnwb6Yhs83zM30WXTSH4thv1bBGv4ZJrCpC28NMh9GzW+sHnZrYNeIWh//ONkcc9Mc8z5+84STeETTdbCD7Q25V7lNVgOTCzHeHDuE7zqcCzZtaX43UGSfqygmbizWE5mMAb5edsgpaDP4bNYB8Kl98KLANul/SCpG9IShXaV5bpBDXEDZEyeANBTSaj6LJjZtsJPuD/IXzNeyQdGK7OLqfP8kaSK0Z2HFMJWlWyTQeOzryf8D2dCbypyP0M4wmmsT5O0NR1PEHBmBEuVxF/+zxBdbw98tNmZr+ObBM3VXZ02QsEJ1XUNCBd4DVccm0AOiRFz6FpkcdD/ueSxhM0j0T/58W6iODb9NFmtgdB7QmKO3/zeR6YpgId0mF/yz8BHwMmmlk7sDmzfzN70szOIPjQvwq4S9J4M+s1s6+a2UHAO4APAZ8qI8bXCWpHmfK3h5lF+1BLKjtmtszM3kfQPPZH4HvhquxyOo2g+W0jsJ2gNggEX1qBvbNfOib2/XO8p//J+kzZzcw+X8r7iPIE01i7E5ykrxCcJP+7hL/9LnCxwkEBYYfj35W4/3uBt0j6uKRdJJ0GHAT8rMTXccnxG4IPn3+UlJL0EeCoyPrbgM9IOlzSrgTn3CNmtq6Mfe1OUKPplrQncFlloQ/6LUGiXChpfNgpf2yO/fcRNqdJupSg7wgASZ+QtLeZDRA0PwEMSDpO0iHhh/EWguamgVICNLMNwH3A1ZL2CDvi95f07lx/ExkkMCNm3WRJp4QJ/3WC5vdMTLcBF0raT9JuBP+zxWEN70/AWEkfDGthlxD0reVzE/A1SQcocKikvQjK/VskfTI8d1KS3i7prSUcmiE8wTTWDwmqu2mCztOHi/1DM/sJwbey28PmiT8AJV13YGavEHx7u4ggyf0T8CEz+0spr+OSw8x2Ah8h6AN4laDZZUlk/X8D/wrcTfAhvj9wepm7u5ag0/gvBOfuf5Ubd5SZ9QMfBv6KYJDJeoL3kW1ZuM8/EZSj1xjaHHQisEbSNuA64HQz6yFo8rmLILk8AfwPQbNZqT5FMKjicYK+n7sIah+5TOWN8p6tBfgSQW3lVeDdBAMhAG4O43sQeIbgfZ4PYGabCfp1bwpfdzvB8crnGuAOggS5haBfrs3MtgLvJzgfXiBo0ryKwgkrJw1tqnXOOVcLki4BXjazGxodS714gnHOOVcT3kTmnHOuJjzBOOecqwlPMM4552rCE4xzzrmaaNgMn5MmTbIZM2bErtu+fTvjx4+vb0B5eDz5jYR4VqxY8Rczy74ArSl4WSqfx5NfxWWp3DlmKv058sgjLZcHHngg57pG8HjyGwnxAMutQed6rX+8LJXP48mv0rLkTWTOOedqwhOMc865mvAE45xzriYSlWA6V6Y5duH9rE5v5tiF99O5spwJXp1zziVBIu4TDUFyuXjJanp6+2EqpLt7uHjJagDmzuoo8NfOOeeSJjE1mEXL1gbJJaKnt59Fy9Y2KCLnnHOVSEyCeaG7p6Tlzjnnki0xCWZKe1tJy51zziVbYhLM/BNm0pYaeivvtlQr80+Y2aCInHPOVSIxnfyZjvygz2UrHe1tzD9hpnfwO+fcCJWYBANBkpk7q4Ouri7OP3NOo8NxzjlXgYJNZJJulvSSpD8U2O7tkvokfbR64TnnyuHXlLkkKKYP5hbgxHwbSGoFrgLuq0JMzrkKZK4pS4cjMDPXlHmScfVWMMGY2YPAqwU2Ox+4G3ipGkE558rn15S5pKi4D0ZSB/C3wHHA2wtsew5wDsDkyZPp6uqK3W7btm051zWCx5Ofx5Msfk2ZS4pqdPJfC3zFzAYk5d3QzG4EbgSYPXu2zZkzJ3a7rq4ucq1rBI8nP4+neJJOBK4DWoGbzGxh1vp5wCIg0571bTO7qZR9TGlvG2wey17uXD1V4zqY2cDtktYBHwX+Q9LcKryuc00l7Ku8HjgJOAg4Q9JBMZsuNrPDw5+Skgv4NWUuOSquwZjZfpnHkm4BfmZmnZW+rnNN6CjgKTN7GkDS7cApwOPV3IlfU+aSomCCkXQbMAeYJGk9cBmQAjCz79Y0OueaSwfwfOT5euDomO1OlfQu4E/AhWb2fPYGhfoz24F/O6aFbdta+beOFtj8JF1dT1bnXVQgaf1jHk9+lcZTMMGY2RnFvpiZzSs7EuccwE+B28zsdUmfA34AvCd7I+/PrA6PJ79K40nMXGTOjQJpYGrk+b680ZkPgJm9Ymavh09vAo6sU2zOVZ0nGOfq51HgAEn7SRoDnA4sjW4gaZ/I05OBJ+oYn3NVlai5yJxrZmbWJ+k8YBnBMOWbzWyNpCuA5Wa2FPhHSScDfQQXOM9rWMCu6XWuTHP50jV09/QCMHFciss+fHDVBoR4gnGujszsXuDerGWXRh5fDFxc77jc6NO5Ms38Ox+jd8AGl23a0csFi1dxweJVQbJ5e/5rGwvxBOOcc6NI58o0i5atjb0YN2rTjl7Wb+qnc2W67BqNJxjnnGtimYTyQncPY1Mt9PQOFP23ZsaiZWs9wTjn3GgVTSJTIhfWZmbWzkx+WkpyyahkDjtPMM45l2C5kkd0fTSJpLt7mH/nY/zzkt+zo4yEkq2SOew8wTjnXELFJY+Ll6wGhk4JlH17ht4BG9J5Xy5JFc1h5wnGOefqpFBtJFuue/tcvnTN4N/V6jYME8el2HfimIqGLHuCcc65OihUG+lcmWbji1v5zIJ7BpNPruTR3dPLrCvu44OH7kOLRL9VXlsB6Ghv46EFb8xMVOm8aH4lv3PO1UG+O41mks/O/gGMN5JP+7hUztfbtKOXHz38XNWSSy1u6eA1GOecq1AxTV/57jT61Z+uiU0+2cuqqb0txfhddym6ua4cnmCccy6PckZxZXfEQ+47jUJQG6mntlQrl59cvSlhcinmfjA3Ax8CXjKzt8WsPxP4CiBgK/B5M3us2oE651y9lTuKq6e3f3DKlfa2FFKQRARkN2hVp4ErXntbilWXvb/kwQXVUkwN5hbg28APc6x/Bni3mW2SdBLBPSribqLknHMjRufKNBfd8diwPo5MvwlQ1JQrmYkkobbJJFumlgJBMmzEHU2LueHYg5Jm5Fn/68jThwnuceGccyNWpuaSqwM9U5OpZR9JOcaPaWXHzv661lLyqXYfzNnAz6v8ms45Vze5ai7ZkpZcrj3t8IYnlGxVSzCSjiNIMO/Ms03e+4hnNNt9qavN48kvafG4ZIq77gTIW3OpBwFtqZbYaV7i+nAguH4lackFqpRgJB1KcHvXk8zslVzb+X3Eq8PjyS9p8bjkyTSBnXvgAEYL6e4eLli8ihZBFWZYqdjjXzsJGD6C7bgD9+buFekhtadaXL9SLRUnGEnTgCXAJ83sT5WH5JxztRU38guSkVyik0vGdc7Pnr5nQ0aElaOYYcq3AXOASZLWA5cBKQAz+y5wKbAX8B+SAPrMbHatAnbOuVJl3xo4qVqKmFyyUSPCylHMKLIzCqz/LPDZqkXknHNVFHdr4HrK1W8Cw6+m75jYP2KSRzH8Sn7nXNOIu6Bw0bK1NU0umQspu3f0DvaTPPDHl4fEAAwb1hx3NX2zDU7xBOOcawpxV91fuHhVTS9uzJ59uJCR0ndSLZ5gnHNNIa7jvpbJpdTRWyOp76RaPME450asaJNYrZJJq8SAGROymsJGQw2kUp5gnHOJlz0KbOK4FB88dJ9h14RUIq4zvi3Vytc/cognkjJ5gnHOJVrcKLDMzbbK0ZZq4bXegSHJJJNIYPT1k9SSJxjnXKJVexTYa70DfPO0w8MZkbfSkZVIPKFUjycY51yi5boTZLmmhPN2zZ3VQVdXF+efOaeqr+/e4AnGOZdImQ78anbeJ3nermbU0ugAnBtNJJ0oaa2kpyQtyLPdqZJMUlNPu9S5Ms2xC+9nvwX3cOzC++lcmR5cfvGS1QVv5lXIuFQLE8elEME1K95hX19eg3GuTiS1AtcD7wPWA49KWmpmj2dttzvwReCR+kdZP/luR5xrMspiTRyX4rIP1/6e8y4/TzDO1c9RwFNm9jSApNuBU4DHs7b7GnAVML++4dVXrnvZZ0ZxlSO7w941licY5+qnA3g+8nw9cHR0A0lHAFPN7B5JORNMM9y87/SpW2Fq3FZb2WW66MsxcizuehVJ7Duxjfa2Ftj8JF1dT5YcTxI0WzyeYJxLCEktwDXAvELbjuSb93VPOIBFy9aS7o7/+Mk3+zAEk0sCQy66LLc5LInHp5ni8QTjRpy4GXNHSJNImqHf2fcNl2XsDrwN6ArvrfQmYKmkk81sed2irKHunl4u/uXqvP0rhUaNbe7p5ZmFH6xuYK4mirnh2M3Ah4CXzOxtMesFXAd8ANgBzDOz31U7UOcgvmP4gsWruGDxKiD8Nvt2NTLEfB4FDpC0H0FiOR34eGalmW0GJmWeS+oCvtwsyQVg4+bX6OmtbPBq9I6PLtmKqcHcAnwb+GGO9ScBB4Q/RwPfIatd2blSxd2L/IE/vlxw2OqmHb2s39RP58p04mo1ZtYn6TxgGdAK3GxmayRdASw3s6WNjbD2dvYPUMnVEX4dy8hSzB0tH5Q0I88mpwA/NDMDHpbULmkfM9tQpRjdCFRMM1bcNsCwW9umu3tKmnfKzFi0bG3iEgyAmd0L3Ju17NIc286pR0z1tEtL+bVLH3o88lSjDyZuZEwH4AlmlMp3fUPmwyFXU1e1VHt6EVe5zpVp+gfi1+Xr2PfEMnLVtZO/GYZWJkG94+nu6WXj5tfY2T/AmNYWJk8YO2QkT99rO/jWj/9zcN3Gza9x7oHZnyR9rH98BZ0vBpd8rH+1h3MPrM0dPCa3wYLDBxL1PxuNsmuoO3b2cdb+8f/zuKUCzjxmGlfOPaSmcbraqUaCKTQyZtBIHlrZjPEU24wVjPppIdN23pbq5+sfOQiAi3+5mnMPhKtXZ06lneF2udrZd4a/WyuOP5cvH9rPvm89gjn+jbdh4mqopWiVuPpjh3mtZYSrRoJZCpwXXpV8NLDZ+1+SIV8CKaYZC3JfbX3RHY8xNtVStZs9VcvEcSn2nTjGP5garNKpXgbM/H/YBIoZpnwbMAeYJGk9cBmQAjCz7xJ0WH4AeIpgmPJnahWsKyyTVNLdPUPatTMJZPmzr+YcjdXT2z845Lc9vD3sph29w7YD6Ddj+87GJ5e4Ow5601jjVdoH5kORm0Mxo8jOKLDegC9ULSKXV+fKNBtf3MpnFtwzpFaSfUtZGN6u3dPbX/RorOjrJMm4VAu7plr9vugJk11bHjemtewvID4UuXn4lfwjSKZZ69wDBzBaBkde/fOS39M7YPT216bTvBEy91x/4I8vj8Qr9keNuC82xfa3TByXontHLxPC2rJ/aWg+nmASLLvwtgji5v/b0Ztj7OcIFNfk5ZIpux+vVOPG7MLKS99f5ahckniCSYBcFxzOv/OxIfcir+JtyROhVeKMo6d6LWWEqrQj369Van6eYBosbjTXhYtXVfU2sfUQTRbp7h5aJfrN6Ainebl7RXrIh5HXVEY+78h3hXiCqaHsJq7MFcnAYI2lJfwgjhppyaWliGsWZk/fc6TOgDzqRWvY0f6SgvPq5+Ed+aODJ5ga6VyZHtbEtWlHL1+6YxWtLRrskM9OLvXSntWxmplMMvohsmlH77DPkLZUK6ce2TGkWatjYn/BZDF3VocnlBEou4Y9ZHRhkadudMAGbPW7To4inmBqZNGytUOSS8aAwUADR3uV2jRVzNX+ft1J86q0nwUY0pHf1dXF+WfOqTAqN1J4gilDoSvkMxc61kNcK0WqRew2dpeqDAH1msfIFz0no31jxZwLlfazdHg/y6jmCaZE+aZYASoatlmqVKtY9NHDALx/w8XKvoNkpkk219RA2aa0t5X9Zcn7WZwnmBLlmptr0bK1g4+rLdPv8bPHNuS8D7knFBcn3x0kM+dtvnNn/gkzi76NQrTm7F90HHiCKVqhpq9qNonlmg4lM215V1cXK70d2xWh0B0k8zWBZc75Yi36O5/92A3lCaYI2c0MteI3VnLVNqY1/+2Jc12Lcknnan788HNFj0LuaG/z89YN4wmmCBu6e+jprd39S3zYpquVyRPG0pbqj/1yJOC4A/cefF7sAJW4oeve1+LieILJkj1C7LgD92ZyleZoiRZMr624emhvS/H1jxwUmzgMuHtFmtnT9wSKG6Ai4JunHe6DSlxRPMFExI0Q+/HDz/GlKtyx1adGcY2SGWp+7ML7hyWZUgeoTAmbwvw8dsUoKsFIOhG4juA+tzeZ2cKs9dOAHwDt4TYLzOzeKsdadXH3DM8uZJXUXTI1Fm8Cc43S3dPLsQvv54Xunpzncr51UQJvCnMlKeaOlq3A9cD7gPXAo5KWmtnjkc0uAe4ws+9IOojgLpczKgmsmCvIK3ntcu9hUUhmSn1PKq7ROlemSW/qId2dv/9wbKqF13oHCiaZM4+Z5uezK0kxNZijgKfM7GkASbcDpwDRBGPAHuHjCcALlQSVPWorc2Otr/50TcX9FpXewyKfjvY2Hlrwnqq/rnPlWLRsLadPLVw36SnifkLtbanBYfLOFauYBNMBPB95vh44Omuby4H7JJ0PjAeOrySoXBeHbdrRW9TVx5B7Bti42YurwUfSuKR5obsHplb+Om2pVi4/+eDKX8iNOtXq5D8DuMXMrpb018Ctkt5mZkO+Gkk6BzgHYPLkyTknSZw4ZoCLDsn1raqPjWt/R9fmJ3MG093TS3pTT/DtbWrwN5WY3AYXHTL0NVpbhIC+AWNMawuTJ4yhffOTdHXljqtatm3blqgJJj2eZAqucdla0Wt4U6+rRDEJJs3Q70H7hsuizgZOBDCz30gaC0wCXopuZGY3AjcCzJ492+bMmRO7w+/ctpSrV+duNxbwzML4vwXC0TLVu27lokP6uHp1cKiS0AzW1dVFrmPXCB5PMs0/YSbpJ1YMWVbsLVx81KOrhvyX+QYeBQ6QtJ+kMcDpwNKsbZ4D3gsg6a3AWODlcgLqXJmmv0CT8IS2VN71tboVqzeDuZFk7qwOOia20dHehgi+HJ15zDTaUvm/fLVKnlxcVRSswZhZn6TzgGUEQ5BvNrM1kq4AlpvZUuAi4HuSLiT4gjTPrLyOjmI6Jrfv7KNzZXpYAcj0u1Sjh6UjchMuv0mSq5Yihvz/A/AFoB/YBpyTNWKzJO1tKR5aMGfIstnT9xw2ijJqwMzPc1cVRfXBhNe03Ju17NLI48eBY6sRUDEdk739NjgLbLXvvxLXNOA3SXLVUOSQ//9rZt8Ntz8ZuIaw+blaMhdKzrriPjbtGJ5kcs1P5lypimkiq6tiT+50dw9nfu83XLh4VUXJJdUiJo5LDTYheNOAq6HBIf9mthPIDPkfZGZbIk/HU9m1vnld9uGDhzWXeTOwq6bETRUT1zGZy0N/frWiffl8YK7Oihnyj6QvAF8CxgA1G1GSOe99XjFXK4lLMHNnddD54uN0tLcOXsOyfWcfvRXex15A+7iU3wzJJZ6ZXQ9cL+njBLNkfDp7m2KH/Bcast0O/NsxLQSVJaDGQ+2TNoTc48mv0ngSl2BgeMdk58p00XfVi+NDLl1CFDPkP+p24DtxK4od8p+0IdseT37NFk/i+mDizJ3VQUeZHY/er+ISpOCQf0kHRJ5+EKj9lbvO1UgiazBx5p8wkwsXryqqx9OHFLskKnLI/3mSjgd6gU3ENI85N1KMmAQzd1YHy599lR89/FzOba497XBPKi7Rihjy/8W6B+VcjYyIJrKMK+cewsRx8Vfx+z3BnXMuWUZUggEfu++ccyPFiGkiy/Cx+845NzKMuAQDb0x14ZxzLrlGXBOZc865kcETjHPOuZrwBOOcc64mVOZtWyrfsfQy8GyO1ZOAv9QxnEI8nvxGQjzTzWzvRgRTa16WKuLx5FdRWWpYgslH0nIzm93oODI8nvw8nuRK2rHwePJrtni8icw551xNeIJxzjlXE0lNMDc2OoAsHk9+Hk9yJe1YeDz5NVU8ieyDcc45N/IltQbjnHNuhPME45xzriYSl2AknShpraSnJC1owP6nSnpA0uOS1kj6Yrj8cklpSavCnw/UMaZ1klaH+10eLttT0i8kPRn+nliHOGZG3v8qSVskXVDPYyPpZkkvSfpDZFnssVDg38Nz6feSjqhVXEnkZWlYPIkoR+F+R0dZMrPE/BDc5e/PwJuBMcBjwEF1jmEf4Ijw8e7An4CDgMuBLzfouKwDJmUt+wawIHy8ALiqAf+rF4Hp9Tw2wLuAI4A/FDoWwAeAnwMCjgEeacT/r0HnjJel4fEkrhxF/ldNWZaSVoM5CnjKzJ42s53A7cAp9QzAzDaY2e/Cx1uBJ4AkTt18CvCD8PEPgLl13v97gT+bWa4ryGvCzB4EXs1anOtYnAL80AIPA+2S9qlPpA3nZak4jS5H0MRlKWkJpgN4PvJ8PQ08ISXNAGYBj4SLzgurhzfXqyodMuA+SSsknRMum2xmG8LHLwKT6xgPwOnAbZHnjTo2kPtYJOp8qrNEvfeElKUkliNo4rKUtAQT572Srqz3TiXdBzwAXGBmW4DvAPsDhwMbgKvLeM15kn5VRjjvNLMjgJOAL0h6V3SlBXVYk/RzSZ+O7O9KSX+R9KKkaZK2SWqlQpLGACcDd4aLKj421ZI5Fo3a/0hWwflZ6HV3A+6myLJUqzh4oxx1A5dkylGmnISxUK1yUoxmL0tJSzBpYGrk+b7AjnoHISlFcGC/ZWZLAMxso5n1m9kA8D2CJoi6MLN0+Psl4Cfhvjdmqqjh75fM7CQz+0G4bBpwEUG7+5vM7Dkz283M+qsQ0knA78xsYxjXRuCXwFnU+diENkr6P5J+lDkW4fK48yld59gaJRHvPSxLdwM/bnRZipSj9wI3h/t9hbCcENSwNlaxnBRjWFlq1OdMaNjnSri8rPMpaQnmUeAASfuFmf10hlbLak6SgO8DT5jZNZHl0fbGvwX+kP23NYpnvKTdM4+B94f7XgpkaiufBv4z60+nAa+ESanSGLLvfHoGkSp9NY5NzD5KsZTgGx8MPRZLgU+FI2COATZHqv/NLq4sLa1nAEkqS3nK0SPAzrCcxJWjUvZRzjmcxLIU97lSXlmqx2iFEkc2nAe8BgwAvyfonLwysv7vgacIOqeWAlMi6ww4F3gS2Ap8jaC6+WtgC3AHMCbcdiLwM+BlYFP4eF/gneHrbAOeA1YB1xBk8peBPmA78PE872EqsCTc/hXg2+HyecCvIttdR5BAtwCQLydqAAAb5ElEQVQrgL+JrDsKWB6+j97wtdYAlwE/Ct9/b3isHgT2BLqAzwLHAz3hMdwG3ALMCN/XLuHrTyAo/BsIvolcCbRG4nwI+GYYf/T4jw+XTYgsWx2+9kB4fG4u4v1dDtwVvpctYdxtBB2Lmwg6hP8JWB/5mynh6/WH++sGzgb+Lty3hfv/Q7i9gOsJRlOtBmY3+vyuc1n6AMHIrT8D/1Kj83MLsBG4Jlw+NvyfvhKeuwY8TlCOVoUx3Rr+P54Iz79XahxHN8FnyRqC0XTbw/efKSeZc/cF4DCqUE7y/E/mAU+Hx2Zd+HsCwZf9SwjKa19YBu4lGIk3h0g5CF9nHXB8nrLUCvxz+L/fGh63qeH2BwK/AF4P99VH0KdyNrAXQWvEk8B/A3tWUpYaXgiyDtoYgvtaXAikgI8SfIheGa5/D8G9CY4AdgW+BTwY+XsjyLh7AAeHB/CXBEM1JxCc6J8Ot90LOBUYRzCE8k6gM/JaXcBnIydFL0FyawU+H56MinkPreFJ/E2CD+OxBG2/mdeJFpxPhHHsQlBNfxEYG677DfDJ8PFuwDHh488BPw3jbgWOBPaIiXnIScnwBPMT4IYwxv8F/Bb4XCTOPuD8MLa2Iv53g/su8v1dHh7TuQSFqw1YCPwPQfLfl+BDYX24fQtBIbmU4Dx5M0FBPSHyej9q9Dmc9J9Gnp9JiaNR5SR8jS3AzPD5PsDB4eOzCL44vzl8H0uAW+NiDJetY2iCyS5L8wkSwUyC5HBYeAzHEyTrz4QxzyL4TK3JEPaGn/BZB+1dZH1wE9Q+Mgnm+8A3Iut2Cw/sjPC5AcdG1q8AvhJ5fjVwbY59Hw5sijyPnoTzCIZ8ZtaNC/f1ppjX+WuCb4a7xKybR6TgxKzfBBwWPn4Q+CrDx+2fFR6TQ2P+vqiCQzAy5PVogSCoqj8QifO5Ev93g/su8v1dTuTLQbhsMGGEzz/LGwnm6OyYgIuB/y/yep5gCv+fGnZ+JiWORpUTgg/3boIvtm1Z634JnBt5PpPgs22X7BjD9esYmmCyy9Ja4JSYGE4D/l/WshuAy2pxviWtD2YKkLbwXYeezVo/+NzMthFUTaPD5TZGHvfEPN8NQNI4STdIelbSFoITtT3P6JEXI/vNDDzYLWa7qcCzZtaX43UGSfqypCckbZbUTVDLmhSuPht4C/BHSY9K+lC4/FZgGXC7pBckfSPsSC3FdIIa4gZJ3eG+byD4hpZRcd9XgfcXt48pWcuij6cDUzLxhq/3zzRmWOlIlpTzMylx5FPVcmJm2wk+4P8hfM17JB0Yrh7y2RY+ziS5YmTHMZWgOSvbdODorHJ0JvCmIvdTkqQlmA1AR9g5mDEt8vgFggMEDHbW7UV5o2MuIviWcLSZ7UFQe4KgOlmJ54FphTraJP0NQR/Dx4CJZtYObM7s38yeNLMzCE7mq4C7JI03s14z+6qZHQS8A/gQ8KkyYnyd4Ftfe/izh5kdHNnGcvxtLkO2L/T+cuxjA0HTWEZ01MrzwDOReNvNbHczy0ylUWq8o1VSzs+kxJFP1cuJmS0zs/cRNI/9kWCkGGR9thF87vURfEHeTtBqAkD4JTj7lsXZcTxP0P8c957+J6sc7WZmny/lfRQraQnmNwQH9R8lpSR9hKHD9G4DPiPpcEm7Av+bYMqCdWXsa3eCGk23pD0JOs+r4bcEH5QLw5ErYyUdm2P/fYTNBJIuJeg7AkDSJyTtbcFwxe5w8YCk4yQdEp5kWwiq0QOlBGjB6I/7gKsl7SGpRdL+kt6d628kzZBkCi6Yi7ORoP24qPeXwx3AxZImSuogGPCR8Vtgq6SvSGqT1CrpbZLeHtn/DElJO6eTJinnZ1LiyKna5UTSZEmnhF+MXyfo0M/EdBtwoYJRf7sRfLYtDmt4fwLGSvpgWAu7hKAPOp+bgK9JOkCBQyXtRTCY6S2SPhl+xqYkvV3SW0s4NEVLVGG0YEqLjxC0bb5KUJ1cEln/38C/Eoyr30CQoU8vc3fXEnSG/QV4GPivcuOOsmD8/IeBvyIYhbae4H1kWxbu808E1eHXGFrNPRFYI2kbwSia082sh6AqexdBoXmCoFP81jJC/RRBZ/njBG3adxF8q8plahhnrtridcBHJW2S9O9FvL84VxAcr2cIRrDcRVAQM8f1QwR9Zc8Q/N9uImgugTcuVHtF0u8K7GfUSsr5mZQ4ilDNctICfImgtvIq8G6CAUMQXJdzK0FT/TME7/N8ADPbTDA69qbwdbcTHK98riH4wnYfwTH4PkG/z1aCIdqnh3G8SFDzK5SwyuI3HHNFkXQJ8LKZ3VDHfX6e4AMj5zdG55KkEeUkyTzBuMRQcJHZmwmaSg8A7iG4NuLahgbmnCtLJVd8OldtYwhG6exH0J5+O/AfDY3IOVc2r8E455yriUR18jvnnGseDWsimzRpks2YMSN23fbt2xk/fnx9A8rD48lvJMSzYsWKv5hZ9rUDTcHLUvk8nvwqLku1mB6gmJ8jjzzScnnggQdyrmsEjye/kRAPsNwadK7X+sfLUvk8nvwqLUveROacc64mPME455yrCU8wzjnnasITjHPOuZrwBOOcc64mPME455yrCU8wzjnnasITjHNNqHNlmmMX3s/q9GaOXXg/nSvLuSefc5XxyS6dazKdK9NcvGQ1Pb39MBXS3T1cvGQ1AHNndRT4a+eqx2swzjWZRcvWBskloqe3n0XL1jYoIjdaeYJxrsm80N1T0nLnasUTjHNNZkp7W0nLnauVRCUY75h0rnLzT5hJW6p1yLK2VCvzT5jZoIjcaJWYTn7vmHSuOjLlJehz2UpHexvzT5jp5cjVXWJqMN4x6UYDSSdKWivpKUkL8mx3qiSTNLuc/cyd1cFDC97DIR0TeGjBezy5uIZITILxjknX7CS1AtcDJwEHAWdIOihmu92BLwKP1DdC56orMQnGOybdKHAU8JSZPW1mO4HbgVNitvsacBXwWj2Dc67aEtMHM/+EmW/0wYS8Y9I1mQ7g+cjz9cDR0Q0kHQFMNbN7JM3P9UKSzgHOAZg8eTJdXV2x223bti3nukbwePJrtniKSjCSTgSuA1qBm8xsYdb6ecAiIDPs69tmdlMpgXjHpBvtJLUA1wDzCm1rZjcCNwLMnj3b5syZE7tdV1cXudY1gseTX7PFUzDBRNqN30fwjetRSUvN7PGsTReb2XllR0KQZObO6qCrq4vzz5xTyUs5l0RpYGrk+b688aUMYHfgbUCXJIA3AUslnWxmy+sWpXNVUkwNZrDdGEBSpt04O8E4VxedK9MsWraWF7p7mDKyarqPAgdI2o8gsZwOfDyz0sw2A5MyzyV1AV/25OJGqmI6+ePajeNK86mSfi/pLklTY9Y7V7HM9VLp7h6M4HqpCxev4pLO1Y0OrSAz6wPOA5YBTwB3mNkaSVdIOrmx0TlXfdXq5P8pcJuZvS7pc8APgPdkb+Qdk9UxmuPZ+OJWzj1wYPiK7U/zrR8/zZjWFiaPI1HHJ8rM7gXuzVp2aY5t59QjJudqpZgEU6jdGDN7JfL0JuAbcS/kHZPVMRrjyTSLpbtbKFTxnn9oP0w9YKQ0mznXtIpJMHnbjQEk7WNmG8KnJxNU/90oVm4/yRuJpIdWiX4z2ttSbN/ZR2+/FbXvATMWLVvrCca5BiuYYMysT1Km3bgVuDnTbgwsN7OlwD+Gbch9wKsUMczSNa8h88qRe1657CR03IF7c/eK9ODf9VuQULp7ekuOwWeAcK7xiuqDKdRubGYXAxdXNzQ3UuWbVy6TYOKS0I8efq5qMfgMEM41XmKminHNo5h55eKSUDX5DBDONV5ipopxyVVqf8qU9jbSMUmmRRq8x0/c+moQsNf4Md7/4lwCeIJxeRXqT+lcmWbji1v5zIJ7BpNP3LxyEPSpXLB4Fa0tqmqMrRIDZoP7b9/8ZFVf3zlXHk8wLq9C9+m5eMlqzj1wAKOFdHcPFyxexcRxKU49soPbHnl+sKM+qn+guNFgxWhLtfL1jxwypMbS1eUJxrkk8AQzihXT9JWrPyXd3cO//GR4LQVg045efvzwc1QvjbxhXKqFXVOtdO/oHWnTxDg36niCGaWKHUqcqz8FYPvO3J30lSYXAe/Yf0/WvdIzEuccc87hCaap5auh5Gr6uuiOx7hw8SomtKWQgtqIqDxhlKJV4uqPHebJxLkRzhNMk8pVQ1n+7Ks88MeXc9ZK4i5urGdyietTcc6NTJ5gmkTcFCvZenr7q3oxYzWkWsRuY3fxPhXnmpAnmCaQXVuJSy5JtejvvCnMuWblV/KPMJ0r06x9cSv7LbiHYxfeP1hzqeVV8cVolfjEMdNob0sNW5frqpeO9jZPLs41MU8wI0imprKzf2DwZlsXLF5Vs6viSzFgxpVzD2HVZe/n2tMOp6O9DREkkTOPmUZbqnXI9m2pVp/Oxbkm501kCRcdCdaSo28lCaKTS86d1TGsZjJ7+p4j9TbHzrkyeYJJsEs6Vw+5YDEJySXVIhBD7s3SIhWsjcQlHedcc/MEkwBx16sANbsaPk5meDAwbB6xzHUwHZHYovF2TOz35OGcG8YTTIPFXa9y4eJVjBvTWtPk0h5eSJlreHCh5qyhc3911TBS59xI5QmmweJGgBn5p2GpRDEXMnpzlnOuGjzBNEC0SaweTWDZTVyePJxz9eAJpoY6V6a5fOmawWlXJo5L8cFD9xly3/layb5HiicV51y9eYKpkc6Vaebf+Ri9kXufbNrRW/WpWtrbUrzeNzAkYfl8Xs65JPAEUyOLlq0dklxqIdUqLj/54MH9+TUmzrkk8QRTI7lu1FWOVKs47e1T+dljG4Y0t1324YMHE4knFOdc0niCqZF8N+oqJHr/lWgiuXJucJ1KV1cXK8+cU5U4nXOuVjzBlCHfjbyi0+ZXYt3CD1YjVOecaxhPMCXKd6thGH4VfDmi83o559xI5QmmRLluNbxo2drBx5XwWYadc83CE0yJcnXel9up396WYvyuu/gIMOdc0/EEU6JcnfeZZq1S+l7aUq1cfvLBnlCcc03JbzhWovknzBx28ywIEksxySV6Iy6/GNI518y8BlOk6MixCW0pxqZa2LSjt6TX6Ghv46EF76lRhG4kkHQicB3QCtxkZguz1n8J+CzQB7wMnGVmz9Y9UOeqoKgajKQTJa2V9JSkBXm2O1WSSZpdvRAbr7unl4uXrCYdTk7Z3dPLa70DTBw3/P7zuXjnvZPUClwPnAQcBJwh6aCszVYCs83sUOAu4Bv1jdK56imYYIosFEjaHfgi8Ei1g2y0jZtfix05VmwNxpvDXOgo4Ckze9rMdgK3A6dENzCzB8xsR/j0YWDfOsfoXNUU00Q2WCgAJGUKxeNZ230NuAqYX9UIE2Bn/wDldld5s5iL6ACejzxfDxydZ/uzgZ/HrZB0DnAOwOTJk3Pe9G3btm2JuiGcx5Nfs8VTTIIpWCgkHQFMNbN7JOVMMCOxUHT39DK5DS46pK/kv22R6JjYX/X3kqTjAx5PLUj6BDAbeHfcejO7EbgRYPbs2TZnzpzY1+nq6iLXukbwePJrtngq7uSX1AJcA8wrtO1IKBTZ08Ds2GmctT9cvbq4Q9UiGLDa3tyr2U7CaktaPBFpYGrk+b7hsiEkHQ/8C/BuM3u9TrE5V3XFfGoWKhS7A28DuiQBvAlYKulkM1terUDrIW4amFJ4c5gr4FHgAEn7EZSh04GPRzeQNAu4ATjRzF6qf4jOVU8xHQuDhULSGIJCsTSz0sw2m9kkM5thZjMIOiZHXHKB+GlgSlHNKfpd8zGzPuA8YBnwBHCHma2RdIWkk8PNFgG7AXdKWiVpaY6Xcy7xCtZgzKxPUqZQtAI3ZwoFsNzMmqYAVJogfJJKV4iZ3Qvcm7Xs0sjj4+selHM1UlTHQqFCkbV8TuVhNUb7uFTJF09m+HUuzjk31Ki+kj/76vwtrxVOLq0SA2ZMaEshQfeOXp+k0jnnYozKBNO5Ms3lS9cM3n4YGPI4nwEznvGbgTnnXEGjLsFkjxQrlfezOOdccUbdbMqVjBRLtcr7WZxzrkijrgZT6rUtGRPHpbjsw37vFuecK1bTJpjsDnyJkkaItUpc/bHDmDurg66uLlaeOad2wTrnXBNqygST3c9SbAd+hmAwuTjnnCtPUyaYSvpZBJx5zDRPLq4pdPf0cuzC+wfn1vPh9K6emjLBlHpFfubaFi+Arpl0rkyT3tRDuju4xXe6u4eLl6wG8HPc1UXiE0z27MbFJIAp7W1Fd+a3pVr9ZmCuKS1atpbTp9qQZT29/SxattbPd1cXiU0wcRdDFvoGlklGxSaXVsmTi2taL3T3DJ0HPbrcuTpI5HUw3T29XLxkdWznfOYbWLZMx34pw5AHzDy5uKaV66LgCW2pOkfiRqtEJpiNm1/L20kf9w2snI59vyrfNbP5J8xEaNjy7Tv76Fw57D5nzlVdIhPMzv6BvOujiaFzZZpjF95f8gWUPvuxa3ZzZ3XQGlPCe/uNi+54zJOMq7lE9sGMiSsVIQHHHbg3UPq8Yh3tbT5c040qfQMWu7zfzEeUuZpLZIKZPGEsban+2MRhwN0r0syevmdJzWJ+O2M3GuX7stbT28/lS9d4gnE1k8gmsva2FF//yCF05OgjyXT0lzIU2ZvD3GgUfFlrzbm+u6eXSzpX1zEiN5oksgYDQbV97qwO9ltwD3GV/HR3D4LYdVEd3hzmRrHgy9pBXHTHY/RbfGn50cPPsWTFenZNtfoN9FxVJTbBZOS7aLKY5OLNYm60yySKCxavyrnNjt4BdvQGg2vS3T1csHjV4PY+k7grV+ITzPwTZpZ1gzBvFnPuDXNndfDVn64paUbxjE07egcTTrvfKtyVIJEJJnuCvlOP7OC2R57PWcXP5s1izg132YcP5sLFqwrW/PPJnlkjWtNpEQxYMENGv1lsMmqv8D24kSVxCSZugr67V6SLSi4+r5hzuc2d1cHyZ1/lxw8/V1GSySUzIjpTVuOS0UWH9HHuv/7c+3tGicQlmFwT9GW+FeXi84o5V9iVcw9h9vQ9h83zV0/5+ntyGZdqGfxb8H6hkSJxCSbXBH39ZqRaRG+OC8d8XjHnipMZoXlJ5+qa1WaqLZNYMqL9QuXIJKzPH7iTeQvuGWze8z6m6kpcggmmgdk6bHmmX+XCO1YRV5HxecWcK02mNpO5pqyYYf/NIjthZb635utjKta4VMuQJsDjDtybB/748pDbt8etm9LexvzDhg9mKueWJUmRuAQz/4SZpJ9YMWRZZkRY5qBmjyrzEWPOlSdTm4HhH2SZD79S5/kb7bKbAH/08HOD67ITWHRduruH51/tY8aCe3K+drlNisWIbYZ8+/DJUkuRuAQzd1YHnS8+Tkd7a2zGzvweqRnduaSKJpts0eST+Ra+aUfvqKr1jCSlJJZcf7NpRy/rN/XTuTJd9udr4hIMBO2gDy2Yk3N9voLgnKu+YspcXBLq3tHLhLYUO/v6y/rQc41lZhXdATWRCcY5N/IUlYR+/gs62ltJd/cMjgz1WlCyVXIH1KISjKQTgeuAVuAmM1uYtf4fgC8A/cA24Bwze7zsqJxzTalQ60S2uA5uoKHDrEebSgZQFUwwklqB64H3AeuBRyUtzUog/9fMvhtufzJwDXBi2VE55xy5a0WVNJFHk9aY1hauPe1wAO9jiiGpogFUxdRgjgKeMrOnwx3eDpwCDCYYM9sS2X48o/t/4pxLsGjS6urqYk7WAKJKZJJXtAkwbuqc7H6pzAiuONlT8NQr6U0cl2LfiWMqOi7FJJgO4PnI8/XA0dkbSfoC8CVgDOBTGDsXo4jm5l2BHwJHAq8Ap5nZunrH6cpT6QCkrq4u1p05p+y/L+eamXx/09XVVXYsUMVOfjO7Hrhe0seBS4BPZ28j6RzgHIDJkyfnDH7btm0Vv7Fq8njy83iKU2Rz89nAJjP7K0mnA1cBp9U/WjcSlZPgajkqt5gEk2bo5C37hstyuR34TtwKM7sRuBFg9uzZNmfOnNgX6OrqIte6RvB48vN4ilawuTl8fnn4+C7g25JkVuRU4s4lSDEJ5lHgAEn7ESSW04GPRzeQdICZPRk+/SDwJM65bMU0Nw9uY2Z9kjYDewF/iW7krQHV4fHkV2k8BRNMeJKfBywjaDe+2czWSLoCWG5mS4HzJB0P9AKbiGkec85Vj7cGVIfHk1+l8RTVB2Nm9wL3Zi27NPL4i2VH4NzoUUxzc2ab9ZJ2ASYQdPY7N+LkHhvnnKu2weZmSWMImpuXZm2zlDdaAD4K3O/9L26k8qlinKuTIpubvw/cKukp4FWCJOTciKRGfTmS9DLwbI7Vk8jq1Gwwjye/kRDPdDPbuxHB1JqXpYp4PPlVVJYalmDykbTczGY3Oo4Mjyc/jye5knYsPJ78mi0e74NxzjlXE55gnHPO1URSE8yNjQ4gi8eTn8eTXEk7Fh5Pfk0VTyL7YJxzzo18Sa3BOOecG+ESl2AknShpraSnJC1owP6nSnpA0uOS1kj6Yrj8cklpSavCnw/UMaZ1klaH+10eLttT0i8kPRn+nliHOGZG3v8qSVskXVDPYyPpZkkvSfpDZFnssVDg38Nz6feSjqhVXEnkZWlYPIkoR+F+R0dZMrPE/BBcfPZn4M0E95V5DDiozjHsAxwRPt4d+BNwEMEMt19u0HFZB0zKWvYNYEH4eAFwVQP+Vy8C0+t5bIB3AUcAfyh0LIAPAD8HBBwDPNKI/1+DzhkvS8PjSVw5ivyvmrIsJa0GMziduZntJJj6/5R6BmBmG8zsd+HjrcATBDPcJs0pwA/Cxz8A5tZ5/+8F/mxmuS7wqwkze5DgCveoXMfiFOCHFngYaJe0T30ibTgvS8VpdDmCJi5LSUswcdOZN+yElDQDmAU8Ei46L6we3lyvqnTIgPskrVAwTTvAZDPbED5+EZhcx3ggmMLktsjzRh0byH0sEnU+1Vmi3ntCylISyxE0cVlKWoJJDEm7AXcDF5jZFoKbqO0PHA5sAK6uYzjvNLMjgJOAL0h6V3SlBXXYug0HVDBR48nAneGiRh6bIep9LFxhCSpLiSpH0PxlKWkJptS7Z9aEpBRBgfixmS0BMLONZtZvZgPA9wiaIOrCzNLh75eAn4T73pipooa/X6pXPAQF9HdmtjGMq2HHJpTrWCTifGqQRLz3JJWlBJYjaPKylLQEU8x05jUlSQQz2j5hZtdElkfbG/8W+EP239YonvGSds88Bt4f7js6rfungf+sRzyhM4hU6Rt1bCJyHYulwKfCETDHAJsj1f9m52VpaCxJLEfQ7GWpXiMlShjZ8AGC0SZ/Bv6lAft/J0G18PfAqvDnA8CtwOpw+VJgnzrF82aCEUCPAWsyx4TgNrq/JLg99X8De9YpnvEEN8CaEFlWt2NDUBg3ENw9dT1wdq5jQTDi5frwXFoNzK73+dTIHy9LQ2JJVDkK9930Zcmv5HfOOVcTSWsic8451yQ8wTjnnKsJTzDOOedqwhOMc865mvAE45xzriY8wTjnnKsJTzDOOedqwhOMc865mvj/AT9RCiYvfyGJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.scatter(range(len(final_err)), final_err)\n",
    "plt.title('Final error')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.scatter(range(len(source_err)), source_err)\n",
    "plt.title('domain classifier, source')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.scatter(range(len(target_err)), target_err)\n",
    "plt.title('domain classifier, target')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.scatter(range(len(class_s_err)), class_s_err)\n",
    "plt.title('class classifier, source')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                    wspace=0.35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3X2cHXV59/HPN5sNWQhmgdAVNpGgRjAaDRABjdINPhCwhTR6C4goVsRa0YKU3sktN6X01Sa9UVArxVKlPKgkCjSmikYkrLQoSNIEAoFAACFZHoKSDQmsstlc9x8zJ8yezJwz52nP2Znr/Xrta8+Zx9/vzJxrfnPN78zIzHDOOZcPY5pdAOeccyPHg75zzuWIB33nnMsRD/rOOZcjHvSdcy5HPOg751yOeNB3zrkc8aDvRoykd0v6paRtkl6QdJekdzS7XM7lydhmF8Dlg6TXAD8CPgt8HxgHvAf4QwPWNdbMdtZ7uc0gqc3MhppdDpcd3tJ3I+VNAGZ2o5kNmdmAmf3MzO4HkDRG0kWSnpS0RdL1kiaG43okbY4uTNJvJL0vfH2JpJskfUfSi8BZktok/R9Jj0naLmm1pCnh9IdLui0829gg6SNJhZb0SUkPhct4XNJnisafImmtpBfDdc0Nh+8v6d8lPS1pq6Rl4fCzJP130TJM0hvD19dKukrSrZJeAuZI+qCkNeE6Nkm6pGj+whlUfzj+LEnvkPScpLbIdPMl3VfBNnMZ5EHfjZRHgCFJ10k6UdJ+RePPCv/mAK8HJgDfqGD5pwA3AZ3Ad4EvAqcDJwGvAf4ceFnSPsBtwPeAPwJOA/5F0vSE5W4B/iRcxieBKyQdCSDpaOB64MJwvccBvwnnuwHYG3hLuJ4rKqjLR4F/APYF/ht4Cfh4uI4PAp+VNC8swyHAT4B/Bg4EZgJrzexe4HfAByLLPTMsr8sxD/puRJjZi8C7AQP+DXhe0nJJXeEkZwCXm9njZrYDWAicJiltCvJXZrbMzHaZ2QBwNnCRmW2wwH1m9juCAP4bM/t3M9tpZmuAm4H/lVDuH5vZY+EyfgH8jCAtBfAp4Bozuy1cb5+ZPSzpIOBE4C/MbKuZDYbzpvVDM7srXObvzazXzNaF7+8HbgT+OJz2o8DPwzOoQTP7nZmtDcddB3wMgjMP4ASCg53LMQ/6bsSY2UNmdpaZTQbeChwMfDUcfTDwZGTyJwmuOXWRzqai91OAx2KmOwQ4JkyF9EvqJzjgvDZuoeFZyd1hKqif4MxhUpl1TAFeMLOtKctebFhdJB0j6Q5Jz0vaBvxFijIAfAf40/Ds5iPAf5nZM1WWyWWEB33XFGb2MHAtQfAHeJogIBe8DtgJPEeQ3ti7MCLMUx9YvMii95uAN8SsehPwCzPrjPxNMLPPFk8oaS+Cs4AvA11m1gncCijFOvaX1BkzrrgucQeb4rp8D1gOTDGzicA3U5QBM+sDfgXMJ0jt3BA3ncsXD/puRIQXTy+QNDl8P4Ug5353OMmNwPmSDpU0AfhHYGnYC+cRYHx4QbMduAjYq8wqvwX8vaRpCrxN0gEEPYjeJOlMSe3h3zskvTlmGePC9TwP7JR0IsNz5N8GPinpveGF6G5Jh4et6Z8QXCvYL1zHceE89wFvkTRT0njgkhQf374EZw6/D68jfDQy7rvA+yR9RNJYSQdImhkZfz3wN8AM4JYU63IZ50HfjZTtwDHAPWGvlLuBB4ALwvHXELRE7wSeAH4PfB7AzLYBf0kQyPsIWsvDevPEuJyga+jPgBcJAnSHmW0nCNynEZxdPAv8EzEHkXDaL4TL2UoQbJdHxv+a8OIusA34Ba+erZwJDAIPE1wMPi+c5xHgUuDnwKMEF2rL+UvgUknbgYvD8hTK8BRByukC4AVgLfD2yLz/EZbpP8zs5RTrchknf4iKc9km6THgM2b282aXxTWft/SdyzBJHyK4RrCy2WVxrcF/ketcRknqBaYDZ5rZriYXx7UIT+8451yOeHrHOedypOXSO5MmTbKpU6dWPf9LL73EPvvsU78CjQJ5rDPks955rDPks96V1nn16tW/NbPi36/soeWC/tSpU1m1alXV8/f29tLT01O/Ao0Ceawz5LPeeawz5LPeldZZ0pPlp/L0jnPO5YoHfeecyxEP+s45lyMe9J1zLkfKBn1J1yh4ktEDCeMl6euSNkq6P/KAiZmSfiXpwXD4qfUuvHPOucqkaelfC8wtMf5EYFr4dw5wVTj8ZeDjZvaWcP6vJtxq1jnn3Agp22XTzO6UNLXEJKcA11vw0967JXVKOii8m2BhGU9L2kJwD/T+GsvsnHOuSvXI6Xcz/Ek/m8Nhu4X3AB9H8hN+nHPOjYCG/zgrfF7oDcAnkm76JOkcgtQQXV1d9Pb2Vr2+HTt21DT/aJTHOkM+653HOkM+692oOtcj6PcRPKezYHI4DEmvAX4MfMnM7o6ZFwAzuxq4GmDWrFlWyy/v/Jd7+ZHHeuexzpDPejeqzvVI7ywHPh724jkW2GZmz0gaR/DUnuvN7KY6rMc551yNyrb0Jd0I9ACTJG0G/hZoBzCzbxI8KPokYCNBj51PhrN+BDgOOEDSWeGws8xsbR3L75xzrgJpeu+cXma8AZ+LGf4d4DvVF80551y9+S9ynXMuRzzoO+dcjnjQd865HPGg75xzOeJB3znncsSDvnPO5YgHfeecyxEP+s45lyMe9J1zLkc86DvnXI540HfOuRzxoO+cczniQd8553LEg75zzuWIB33nnMsRD/rOOZcjHvSdcy5HygZ9SddI2iLpgYTxkvR1SRsl3S/pyMi4T0h6NPz7RD0L7pxzrnJpWvrXAnNLjD8RmBb+nQNcBSBpf4Ln6R4DHA38raT9aimsc8652pQN+mZ2J/BCiUlOAa63wN1Ap6SDgBOA28zsBTPbCtxG6YOHc865Biv7YPQUuoFNkfebw2FJw/cg6RyCswS6urro7e2tujA7duyoaf7RKI91hnzWO491hnzWu1F1rkfQr5mZXQ1cDTBr1izr6empelm9vb3UMv9olMc6Qz7rncc6Qz7r3ag616P3Th8wJfJ+cjgsabhzzrkmqUfQXw58POzFcyywzcyeAVYAH5C0X3gB9wPhMOecc01SNr0j6UagB5gkaTNBj5x2ADP7JnArcBKwEXgZ+GQ47gVJfw/cGy7qUjMrdUHYOedcg5UN+mZ2epnxBnwuYdw1wDXVFc0551y9tcSFXOeqtWxNH5et2MDT/QMc3NnBhSccxrwjYjuJ1XXeWjVz3S7fPOhnXD2DS6XLSpq+1jIV5j9tynYu/+laLBze1z/AwlvWAZRd3rI1fSy8ZR0Dg0Ml521EHeLWff7StZy3dC2dHe1I0P/yIBMjrwvr6Ez9KTVP1g6m0WXGbZM0+1orHeAVZGdax6xZs2zVqlVVz+9du17dyfr6BxAQ3cKF992dHcw5/EDuePj5VAENGBaoADra21g0f0ZskIxbd0d7Gx86qpubV/cNW060TElfiLjlXjBjJ19ZF99uKRc8C8sqNe/WlwcTP7/i4e1jxITxY/dYX9zrrS8Pxq63nI72Nha9q415J76/qvkLag1i5ZadZj9JKk9SGXp7e+mfOK3kAbjc/h633HKfBey530dF61bp9wYoue7ObY9WFMskrTazWWWn86A/XJodsNQ0STtRmtfVfumidY770qWVFNCK3xfrTvkFAWiTGCqxz8V9SZPqVCrol9LR3lbV59MKFs7cRddhR1YUtIv3yZde2cngUPw2SPr8057tlDqYxi0zLlDHHUD//A0DXL5ubKpGRJLCekod1OOmL6dQr+J9tNT8nR3t/GHnrtIHkwoP8B70Q5WcWsUFlzQ7SnSaUl+oNIrXl+aA8edvGGDJpn3LfukaKe0XpBKFL39Sy7jaoD+aXTBj5x7BL6rSwFZKUiOg3BlbNcssJ2lbl2tEjGYLZ+7iM6f9aerp0wb9TH9j0uZtCy5bsWGPI29hd+ofGNxjWKlpqhW3rDSvC3VrVgu2EV+7wV1WdSoky0p91mn210rXUzz/wOAQN96zaY9gm2Y9ScusVlYDPsArQ7sastxM308/LogPDA5x2YoNsdM/3YQWcj2N1pRFrdTsAtRoNJY/y8G2VYxra0x4znTQTwriff0DzF68kmVrgrtCLFvTx+zFKxvSWm0FKvrfjHVXO77cfN2dHVxx6ky+eupMOtrbqlwaJedN+vwK7zs72mlvq7wmHe1tfPXUmVxx6ky6OztQuKz99m4f9tpVppH7e3T7NFJHextdE8c3ZNmZTu8c3NmRmN+OdpNrRD66VZS6IFfovVOq10PafG6pdSddZygeX+mFs85tj/L5M3qGjSt1ET0pVRQtR6mLo/W8gF88f6mL97MXr6zrdZq4C6W15P6TlLtYGZX2uljaTgXlLhYnrTvps4jrgVRqu8TNX+pz6EzovdMImQ76cVfUo8rlF9PsKLV03Wv0l07AXQuO3/1+3hHdFXebSxo+65D9S3bNLP6CxHVbKywrKXAWf/mLl9vbO/xLUap+heWnKUeSUtOkmb9apXqGVNoTpdJuscXzJ4nbBy45+S0AFXenjDuAws7d052/dG1sWUrt75V2U03TASRuu8R1xyzXs6m7s2NYuQuK9+96yXTQL2ykanq0CLji1Jk1tf4qVW13T9gZu7yDOztSrzspaKUZXu4ziG6HUp9T8brq/aOWtOVoNdFyw/aq+5xXclCrpDtm8ZlS0llM2u0Zt8/19vbuPqtLKkep/b3Sg3LaRkChPKXqHZXU6BhJmemyGf2VZqH7YtpTsThJR99WtOwnt7Hwl0MV/SAmC/yHeCOrmh9e1Uu536KMlv29koZMpds6V102h+0EU4bn6wutkHKpnqhmHH1r0dnRzqL500ddC9aNLq1yptQq5ahGI9OAaWUi6JfqX1/ov75o/gwWzZ9R9c+1W10r7Ewu+1plP2uVcoxGmQj65frXF/rm37Xg+Irzi845lyWZCPqlumYWFB8YvKXgnMujTPw468ITDiv7w5xKerI451xWpQr6kuZK2iBpo6QFMeMPkXS7pPsl9UqaHBn3/yQ9KOkhSV+XVPcfs807optF82fQHQb24hWMtguzzjnXKGWDvqQ24ErgRGA6cLqk6UWTfRm43szeBlwKLArnfRcwG3gb8FbgHcAf1630EfOO6OauBcczo3visJ+1d3d2jIquXM45NxLS5PSPBjaa2eMAkpYApwDrI9NMB74Yvr4DWBa+NmA8MI6gAd4OPFd7sUvzfL1zzsVLE/S7gU2R95uBY4qmuQ+YD3wN+DNgX0kHmNmvJN0BPEMQ9L9hZg8Vr0DSOcA5AF1dXfT29lZaj9127NhR0/yjUR7rDPmsdx7rDPmsd6PqXK/eO38NfEPSWcCdQB8wJOmNwJuBQo7/NknvMbP/is5sZlcDV0Pwi9xafnHov9LMjzzWO491hnzWu1F1ThP0+4ApkfeTw2G7mdnTBC19JE0APmRm/ZI+DdxtZjvCcT8B3gkMC/rOOedGRpreO/cC0yQdKmkccBqwPDqBpEmSCstaCFwTvn4K+GNJYyW1E1zE3SO945xzbmSUDfpmthM4F1hBELC/b2YPSrpU0snhZD3ABkmPAF3AP4TDbwIeA9YR5P3vM7P/rG8VnHPOpZUqp29mtwK3Fg27OPL6JoIAXzzfEPCZGsvonHOuTjLxi1znnHPpeNB3zrkc8aDvnHM54kHfOedyxIO+c87liAd955zLEQ/6zjmXIx70nXMuRzzoO+dcjnjQd865HPGg75xzOeJB3znncsSDvnPO5YgHfeecyxEP+s45lyMe9J1zLkc86DvnXI6kCvqS5kraIGmjpAUx4w+RdLuk+yX1SpocGfc6ST+T9JCk9ZKm1q/4zjnnKlE26EtqA64ETgSmA6dLml402ZeB683sbcClwKLIuOuBy8zszcDRwJZ6FNw551zl0rT0jwY2mtnjZvYKsAQ4pWia6cDK8PUdhfHhwWGsmd0GYGY7zOzlupTcOedcxWRmpSeQPgzMNbOzw/dnAseY2bmRab4H3GNmX5M0H7gZmAS8BzgbeAU4FPg5sCB8YHp0HecA5wB0dXUdtWTJkqortGPHDiZMmFD1/KNRHusM+ax3HusM+ax3pXWeM2fOajObVW66sTWV6lV/DXxD0lnAnUAfMBQu/z3AEcBTwFLgLODb0ZnN7GrgaoBZs2ZZT09P1QXp7e2llvlHozzWGfJZ7zzWGfJZ70bVOU16pw+YEnk/ORy2m5k9bWbzzewI4EvhsH5gM7A2TA3tBJYBR9al5M455yqWJujfC0yTdKikccBpwPLoBJImSSosayFwTWTeTkkHhu+PB9bXXmznnHPVKBv0wxb6ucAK4CHg+2b2oKRLJZ0cTtYDbJD0CNAF/EM47xBB6ud2SesAAf9W91o455xLJVVO38xuBW4tGnZx5PVNwE0J894GvK2GMjrnnKsT/0Wuc87liAd955zLEQ/6zjmXIx70nXMuRzzoO+dcjnjQd865HCl7752RJul54MkaFjEJ+G2dijNa5LHOkM9657HOkM96V1rnQ8zswHITtVzQr5WkVWluOpQleawz5LPeeawz5LPejaqzp3eccy5HPOg751yOZDHoX93sAjRBHusM+ax3HusM+ax3Q+qcuZy+c865ZFls6bsWJWlH5G+XpIHI+zNGuCzjJZmkySO5XuearV5PznKuLDPb/ew3Sb8Bzjazn1ezLEljw9t+Z1Ye6uhGXmZa+pLmStogaaOkBc0uT6NImiLpDknrJT0o6a/C4ftLuk3So+H//Zpd1kpJmi3pHkn9kp6WdIWkseG4Qst8k6SXgAfCB/tskPRK+Pd1SXdL+lhkmZ8Jp3lB0o8ldYej7gz/bwjPNObFlOdwSb3hvM9Luk7SvpHxUyX9UNJvw7+vRMb9paSHJW2XtE7SjLizC0lLJF0Uvp4b7r//V9JzwFWS3iDpWUk7JQ1JulPSmyPbujdcxrOStkpaGi5ro6T3R9YzXtI2SW+ux7ZqJEnnh/v2A5JuDMt+aLhvbJS0NHyg06gm6RpJWyQ9EBkW+z1W4Oth/e+XVPUTCDMR9CW1AVcCJwLTgdMlTW9uqRpmJ3CBmU0HjgU+F9Z1AXC7mU0Dbg/fjzaDBA/sOYDg2cp/CpxdNM0Y4L8Inrv8NeAQYD5wLcGjPI8qTCjpVOC8cDldwBrgO+Ho48L/h5nZBDNbllCmS4HXAjOAwwgfByqpHfgJwYOFXkfwSNGbw3FnAv8bOB14DfBhYGvKz2Aq0B4u7wvAPxI8q2LfsA7bgVt4dVtPBt4KHB6OvzJczvXAxyLLPQV4xMweSlmOpggPyl8AZpnZW4E2gqf1/RNwhZm9keCz/FTzSlk31wJzi4YlfY9PBKaFf+cAV1W9VjMb9X/AO4EVkfcLgYXNLtcI1f2HwPuBDcBB4bCDgA3NLluZcv8GeF+ZaRYAN4av3wAY8HngRwRPYdsO9Eb3AWAL8LFw2B3AGZHltRMcWLqA8eHyJldQ5tOAX4Wv5xA8K3pMzHS/AD4TM3yPdQJLgIvC13OBl4D28P1E4AnCDhfhsGMJDvwHAYcCrxAE8+J1TQW2AR3h+x8BX2j2dk/xGXcDm4D9CdLPPwJOIPhl6tjotm52WetU36nAA5H3sd9j4F+B0+Omq/QvEy19Xt1RCjaHwzJN0lSCFu89QJeZPROOepYgsI0qkqZL+omk5yS9CFxM8FN0gMvC/8+G/w8gCOBPhe8L27wvsshDgG+G6aJ+4HmCgJnq4q2kgyX9QFJfWJ5vRcozBXjCzHbFzDoFeCzNOmI8a2aD4etDgd8BjxRSWMDPgLZwW08hOMj9UfFCzOw3BGc28xQ8o/p4ggNMSzOzPuDLBNv1GYID12qg3169vpHl73fS97huMS4rQT93JE0gSCecZ2YvRsdZ0BQYjX1x/w34H+ANZvYagtSKJP0JQcAuFhfAo1+ETcBZZtYZ+esws9Wk+3wuI2h5vzUsz9kEZxiFZU+VFPcd2kRwZlLsFYID1d6RYa8tmiZarrHAkcALBAH+KoY/lnQTQcBPqst1BCme04CVZrYlYbqWEeawTyE44B0M7MOeKZBcaNT3OCtBv4/gS1EwmeEtvkwJ88k3A981s1vCwc9JOigcfxBBC3C02RfYZmY7JL0F+HQ4fDbwwfD1lQSt1q8R5HuPkXQSQau+HYhewP4mcJGkwyAIKJI+BGBmfyBoRb6+THl2AC9Keh3wxci4/yZIL/29pL0ldUh6VzjuW8ACSW8PL8C9SdLk8KxgHXCGpDZJJxOkKpJsDtf/DNAP3Fb4HCQdZGZPAHcDYyRNlDRO0nGR+W8C3g18liDHPxq8j+AM6vnwjOcWgu3fqfCiPtn+fid9j+sW47IS9O8FpoVX+McRtGyWN7lMDSFJwLeBh8zs8sio5cAnwtefIMj1jzbnA2dL2kEQ3JcCmNlC4I3hNJ8jaLWeAawE/gX4OsFFrz6CoPqHcL4bgW8At4TpmbUE1z8KLgZ+EKZ/To4pz8UEQXMb8B+EF2rDZQ8CJwFvJwjOTwF/Fo67AbicIOhuD/93hrOeC5xKcDFyHkHOOpaZPQs8TPBl/x3Bhb/7CVp/hW39i7DejxKkAz4bmX878J8ELebR8n14Cjg2PJAKeC+wnuD6zIfDaUbr/p1G0vd4OfDxsBFxLEHj6Jm4BZTV7AsZdbwgchLwCEEu9UvNLk8D6/lugi/9/QRBbG1Y9wMIAt+jwM+B/Ztd1gbVvwf4Ufj69cCvgY3ADwjSJr8F3tnsctaxvjOBVeH2XkZwJpN6WxP0/vlWs+tRYZ3/juBg9wBwA7BXzLbeq9nlrEM9byQ4ixskaDh8KmnbEqQVrwzj2zqC3k1Vrddvw+BGNUknAr8kaN1/iaB19EYze6WpBWsB4QXc+4B5ZvbrZpfHtYaspHdcfh1H0K1xC0Eq4M884IOkcwm6xf7AA76L8pa+c87liLf0nXMuR1ruhmuTJk2yqVOnVj3/Sy+9xD777FO/Ao0Ceawz5LPeeawz5LPeldZ59erVv7UUz8htuaA/depUVq1aVfX8vb299PT01K9Ao0Ae6wz5rHce6wz5rHeldZb0ZJrpPL3jnHM54kHfOedyxIO+c87liAd955zLEQ/6zjmXIy3Xe8c550aLZWv6uGzFBp7uH+Dgzg4uPOEw5h3R2rf696DvnHNVWLamj4W3rGNgcAiAvv4BFt6yDqClA78HfefcqNCsVnXSei9bsWF3wC8YGBzishUbUpWrWfXxoO+ca3n9A4MsvH3kWtWFgNzXP4B49fFV0fU+3T8QO29f/wCzF6+MDeKllnv+0rWct3Qt3eEBoHOPJdeHB33n3IiopWX73LbfMzA4vN9JJa3qSssZTdsU35KysN6DOzvoKxH4CwcHIDbQFy+3+MCy6F1ttVYllgd951xq0cA9saMdCfpfHhwWxOOCO1BT/vuVoV3EdTZMam0XlzXNQSbaCi+nr3+Azo522tvE4FD8nYoHBoe4ZPmD/GHnrsQDSJKBwSGe2zaYcurKeNB3roTR2DujnGrrVNwC7h94NSgVgviqJ1/g5tV9ewT38e1jYvPf5y1dyyXLH4w9eESNa4vvXX5wZ0eqspY7yBRPn0b/wCDtY8R+e7ez9eX4AB39jCoVHOjqz4O+cwniAkch79pZ1Mqdc/iB3PHw8y1/cCgVDIHYg0HaFvDA4BA33rOJoaJndAwMDpUMpnEHD2DYuk+bsgsxZlhLWSTnzyu9yBo3fRqDu4y9x41l73FjU50hVCLpQFcrD/pu1Hk1EGznS5EvfL1b5XGBoBB0igPVd+5+atj7uFblSJ41VNrjpDgNkdRyL6c44FdjYHCIC75/H+ctXftqDnxK8L/wPukiaOFgnNTyLj5IpD2gRddX7On+Aa44deYeZwod7W2Mbx+TWJbocouX39HeRtfEcSXLVC0P+m7E1RL8hrVUp5RPK0D1vTtK5YvLKaQuLluxoWxOGyibJ09Saf48qU5xaYiklnspbVLs9J0d7cMOKuUUlhF3sTNuHXEH4yTRg0SpYF5Q6E2TdHA4uLNj9zYq3o5bXx7cYx2F991lroN0bnu0bF2q4UHfjahaf9CS1FJNSitEA2+lwb9U74y0yuW0i1vYpVIdxZI+y6R1letxEqeSgN/R3saHjure48ygo72NS05+C0DqC6X1KlOSpB40UR3tbSyaP2PYZx/Xmi8caOcd0b07gBf3/okL9FGFeaN6ez3o51aWLiYmBe244BzXUyTpVLlUIEjKxZfrfVLcxa5apXLa5VqmxZ9NtO920meZtK6kNEQpSS13IPaz/O7dTzGxo53x7WNiz1bigmKlSpWpXuKCc3FrPum7mJQW7O7s4K4Fxze03Gl40G9xo/Wn3sXS5E6LUx5JPUXilAsEcaf/aXqfJOWRR1q073YlXQujitMQ5eYv1XKPtoDjevV0tLdxxakzY/fRpFRI/8uDjCmzHZPKVEqlB4lSwTmuRV4sKYVWS7qwnjzot7haf+rdCipp2RXqVnidRjWBIGndcWmi6Gl5Uu690HsnKZDG5bTTXOgrLt/T/a/wz7+svJUcl4Y4dMGPEw9i0ZburEP2L9m6rWYfTQqecfuKSpSp3NlYpftG9HOqVlIKLal76UjzoN/iam01tEJqqNLucJW0iJICQbWSWoRP9w+kauXFBa3inHapi65ly7fLKg74hTTM+UUptKTgVNzSLVfvUvtopftfUgqlc9ujfP6MntgypfnBWKmDRLl8e6UuPOGwkrn/ZvOg3+JqaTU0MjVUyZe50tPaQt3KBe9xbWN2t77PX7q26kBaSZnKKZf3TQpUhRx4uWsXlSo+w4juA6WCUyXbN2kfndjRXtX+V+lFzTQH46SDRCMaQmlz/83iQb/F1dJqKHfanbTzlxte6iZUcTt2UlBISnmkCdwd7W3sO35MbFBZNH8Gi+bPqNvF2EpbaeWCUJoceNIZw9gx8Z9H0mcp7ZkmK+wDhdZ8rbdMSNpHS627mQEwzUFiNKyjWh70W1zSRa/zE3p0RJU77Y77Yif1d4+7wBlVqntkUlBISnkU54uTTtuf2/A/iTfhumvB8WVP/9O0qut1yh+VJgfZTc9QAAANi0lEQVSe1Frk2fV0tA+l/izPX7o2tgyFfSMuOM1evLKiYJ1U1nLrds3hQX8USOr/W+5ufKVSQ5X2d0/7I524VmHalEdSvZP88/pVpLkJV7nlzF68MlVuu17SXqdJSnMsmj899WdZ6gdFtZavXFmrWbdrPA/6o0hSoE66G1+p1FBSKywpsFfS5a2SG2nVotKbcCUZ6QtvtfbuqCR1UE3d6tX7pNUvaOaVPxh9FElqaSXdjW/eEd0smj+D7s4ORNByLfSvrvQL3CaVn6hI/8AgW18exHj1DGDZmr7E6Zet6WP24pUcuuDHzF68suS0AF0Tx9PRPvwsp5qgUupzaoQLTzisLuVOo5q61at8I/25unRqaulLmgt8DWgDvmVmi4vGvw64DugMp1lgZrfWss48S2qBjWsbk3jxNalVGNcKS5LU17nSC6Sl8sLV9DTq7GgvmeqoxEheeBvp3h2V1q2e5WvlC5p5VXXQl9QGXAm8H9gM3CtpuZmtj0x2EfB9M7tK0nTgVmBqDeXNnEq6jyWdLif1YoHSd3n80FHdJX9QBOV/pAOVdY9MOlup9kdorR5UKj0Yt4pWL5+rXi0t/aOBjWb2OICkJcApQDToG/Ca8PVE4Oka1jfqlAvolbZuk1pgpXqxJHUB7Osf4ObVfSyaP4Pzl66NbbELUv9IJ+2PopLSSq3+0/VqZOUWGi5bZFXeuEjSh4G5ZnZ2+P5M4BgzOzcyzUHAz4D9gH2A95nZ6phlnQOcA9DV1XXUkiVLqioTwI4dO5gwYULV89dL/8AgfVsH2BX5fMdIdO/XQWdHOwAbnt0em48f1zaGw167b+p1bXlhG88lxMYZ3RPLrgvirwtUWo64OkcV1z+qms+iVbZ1knpt36hWr3Oj5LHeldZ5zpw5q81sVrnpGt1753TgWjP7iqR3AjdIequZDfsmmNnVwNUAs2bNsp6enqpX2NvbSy3z10vQDXDPrpTdnW3ctaAHgE8u+DEWcy1dwBOLe1Kv61+X/CdfWRd/Tb570y4uPOEwFv90beK6kh4AsWj+DHoqbJGm+Ul8nP6EHyOVKkOrbOsk9dq+Ua1e50bJY70bVedagn4fMCXyfnI4LOpTwFwAM/uVpPHAJGBLDettaeXugBhNVyRdmDWIfQRckqAXS/wtdQsphc6E53jGPQAiza9zk1SbC271n65Xo9VvvOXyqZagfy8wTdKhBMH+NOCjRdM8BbwXuFbSm4HxwPM1rHNEVRrw0txNMvqFL9WDppL8b7QXS1yQGRgcYq+xY+hobyv7AIhS9Wl0TjprFw/r2U896RGRzlWq6n76ZrYTOBdYATxE0EvnQUmXSjo5nOwC4NOS7gNuBM6yai8ijLBCwOvrH0jdz7zc3SSLv/DRfsxxorcZTirj7MUrWde3bfftD5J6028bGKy4z3SpHjWuvHr1U4/ui5BuX3QuSU05/bDP/a1Fwy6OvF4PzK5lHc1STRfCUj1Nyj0mLene5knLTHpWbLk0TiUBJ4s9akZaPc5esvBMBdc6/Be5CaoJeEm52sI9XEp9QZPmTRqeFAjMqNuvPSstk2sMP/i6evJ77yQodRGu1F0b29vE4NCrbfa4+5PH9WqJy/+KoAUfd1E36Qu/bWCQK06dWZcLon7vlNbgF4RdPXlLP0HS/UfmHH7gsFx/9P4y/QODYLDf3u3DcrhA4jzRi6PR/H7c/eqjOdxSrfB5R3Rz14LjeWLxB8ueYZTi905pDSN5rx6Xfd7ST5DUhbDcxdrBXcbe48ay5uIP7B4Wd3/yqOJ7wMfd6rc4hztSrfCs9agZjaL7ImxvyD3+XX540C8hLuAl3ZI4qjj1kib3Gp0mTQ7XA0G+FPbF3t7eYc+Kda5SHvRTKuTk0/Q3LU69JOVkk+ZJm8P1QOCcq5Tn9FMo7iddSlyKJS4nW2oez+E65xrFW/oplMrjd6a4v0zSc26T5sniLQmcc63Bg34KSTl2AWv/9gOx44pV8yALD/LOuXrzoF8k7n473k/aOZcVntOPSLrfzpzDD/Qcu3MuEzzoRyTd2uCOh5/3Hyk55zLB0zsRpfrHe47dOZcF3tKP8BuMOeeyzoN+hPePd85lnad3Irx/vHMu6zzoF/HcvXMuy3IV9Ct95q1zzmVNboL+SD/k2znnWlFuLuT6Q76dcy5HLf1SffA97eOcy4vctPST+tpP7GiPvfVC9NGEzjmXFbkJ+kl98CU87eOcy43cBP2kh3z3vzwYO32aRxw659xok5ucPgzvg1/u8Yd+6wXnXBblKugXFHffLOa3XnDOZVXmg35cz5xSjz/s9t47zrkMy3TQT/pBVlLAF3DXguNHsITOOTeyMn0hN+kHWW1S7PSex3fOZV2mg35SD5whM7+FsnMul2oK+pLmStogaaOkBQnTfETSekkPSvpeLeurVFLLvdBd0x9/6JzLm6pz+pLagCuB9wObgXslLTez9ZFppgELgdlmtlXSH9Va4EpceMJhe+TwCy16v4Wycy6ParmQezSw0cweB5C0BDgFWB+Z5tPAlWa2FcDMttSwvor5Q1Gcc244mSX9PKnMjNKHgblmdnb4/kzgGDM7NzLNMuARYDbQBlxiZj+NWdY5wDkAXV1dRy1ZsqSqMgHs2LGDCRMmVD3/aJTHOkM+653HOkM+611pnefMmbPazGaVm67RXTbHAtOAHmAycKekGWbWH53IzK4GrgaYNWuW9fT0VL3C3t5eapl/NMpjnSGf9c5jnSGf9W5UnWu5kNsHTIm8nxwOi9oMLDezQTN7gqDVP62GdTrnnKtBLUH/XmCapEMljQNOA5YXTbOMoJWPpEnAm4DHa1inc865GlQd9M1sJ3AusAJ4CPi+mT0o6VJJJ4eTrQB+J2k9cAdwoZn9rtZCO+ecq05NOX0zuxW4tWjYxZHXBnwx/HPOOddkmf5FrnPOueE86DvnXI540HfOuRzxoO+cczniQd8553Ikkw9RiXtalt9vxznnMhj0+wcGWXj7nk/LAjzwO+dyL3Ppnee2/T72aVmXrdjQpBI551zryFzQf2VoV+zwpKdoOedcnmQm6C9b08fsxSsTx/vzb51zLiM5/WVr+l59QtaUPcf782+dcy6QiaB/2YoNe+TxC7q9945zzu2WiaCflK8XcNeC40e2MM4518IykdNPytd7Ht8554bLRNC/8ITD6GhvGzbM8/jOObenTKR3Cvn6oC/+ds/jO+dcgkwEfQgC/7wjuunt7eXzZ/Q0uzjOOdeSMpHecc45l44HfeecyxEP+s45lyMe9J1zLkc86DvnXI540HfOuRyRmTW7DMNIeh54soZFTAJ+W6fijBZ5rDPks955rDPks96V1vkQMzuw3EQtF/RrJWmVmc1qdjlGUh7rDPmsdx7rDPmsd6Pq7Okd55zLEQ/6zjmXI1kM+lc3uwBNkMc6Qz7rncc6Qz7r3ZA6Zy6n75xzLlkWW/rOOecSeNB3zrkcyUzQlzRX0gZJGyUtaHZ5GkXSFEl3SFov6UFJfxUO31/SbZIeDf/v1+yy1pukNklrJP0ofH+opHvCbb5U0rhml7HeJHVKuknSw5IekvTOrG9rSeeH+/YDkm6UND6L21rSNZK2SHogMix22yrw9bD+90s6str1ZiLoS2oDrgROBKYDp0ua3txSNcxO4AIzmw4cC3wurOsC4HYzmwbcHr7Pmr8CHoq8/yfgCjN7I7AV+FRTStVYXwN+amaHA28nqH9mt7WkbuALwCwzeyvQBpxGNrf1tcDcomFJ2/ZEYFr4dw5wVbUrzUTQB44GNprZ42b2CrAEOKXJZWoIM3vGzP4nfL2dIAh0E9T3unCy64B5zSlhY0iaDHwQ+Fb4XsDxwE3hJFms80TgOODbAGb2ipn1k/FtTfBwpw5JY4G9gWfI4LY2szuBF4oGJ23bU4DrLXA30CnpoGrWm5Wg3w1sirzfHA7LNElTgSOAe4AuM3smHPUs0NWkYjXKV4G/AXaF7w8A+s1sZ/g+i9v8UOB54N/DtNa3JO1Dhre1mfUBXwaeIgj224DVZH9bFyRt27rFuKwE/dyRNAG4GTjPzF6MjrOgH25m+uJK+hNgi5mtbnZZRthY4EjgKjM7AniJolROBrf1fgSt2kOBg4F92DMFkguN2rZZCfp9wJTI+8nhsEyS1E4Q8L9rZreEg58rnO6F/7c0q3wNMBs4WdJvCFJ3xxPkujvDFABkc5tvBjab2T3h+5sIDgJZ3tbvA54ws+fNbBC4hWD7Z31bFyRt27rFuKwE/XuBaeEV/nEEF36WN7lMDRHmsr8NPGRml0dGLQc+Eb7+BPDDkS5bo5jZQjObbGZTCbbtSjM7A7gD+HA4WabqDGBmzwKbJB0WDnovsJ4Mb2uCtM6xkvYO9/VCnTO9rSOStu1y4ONhL55jgW2RNFBlzCwTf8BJwCPAY8CXml2eBtbz3QSnfPcDa8O/kwhy3LcDjwI/B/ZvdlkbVP8e4Efh69cDvwY2Aj8A9mp2+RpQ35nAqnB7LwP2y/q2Bv4OeBh4ALgB2CuL2xq4keC6xSDBWd2nkrYtIIIeio8B6wh6N1W1Xr8Ng3PO5UhW0jvOOedS8KDvnHM54kHfOedyxIO+c87liAd955zLEQ/6zjmXIx70nXMuR/4/FPnXNmG92cgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.scatter(range(len(source_acc_list)), source_acc_list)\n",
    "plt.title('Source accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.scatter(range(len(target_acc_list)), target_acc_list)\n",
    "plt.title('Target accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
